{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to recommendation engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recommendation engine is <t style=\"color: yellow\">a model that can predict what a user may be interested in</t>. When we apply this to the context of movies, for example, this becomes a movie recommendation engine. We **filter items in our database by predicting how the current user might rate them**. This helps us in **connecting the user to the right content in our dataset**. Why is this relevant? If you have a massive catalog, then the user may or may not find all the content that is relevant to them. By recommending the right content, you increase consumption. Companies such as Netflix heavily rely on recommendations to keep the user engaged.\n",
    "\n",
    "Recommendation engines **usually produce a set of recommendations using either collaborative filtering or content-based filtering**. The difference between the two approaches is in<t style=\"color: yellow\"> the way that the recommendations are mined</t>. <t style=\"color: yellow\">Collaborative filtering builds a model from the past behavior of the current user, as well as ratings given by other users</t>. We then use this model to predict **what this user might be interested in**. </t>. <t style=\"color: yellow\">Content-based filtering, on the other hand, uses the characteristics of the item itself in order to recommend more items to the user</t>. The similarity between items is the main driving force here. In this chapter, we will focus on collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building function compositions for data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major parts of any machine learning system is the **data processing pipeline**. Before data is fed into the machine learning algorithm for training, we need to **process it in different ways to make it suitable for that algorithm**. **Having a robust data processing pipeline goes a long way in building an accurate and scalable machine learning system**. <t style=\"color: yellow\">There are a lot of basic functionalities available, and data processing pipelines usually consist of a combination of these.</t> Instead of calling these functions in a nested or loopy way, it's better to use **the functional programming paradigm to build the combination**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's take a look at how to combine these basic functions to form a reusable function composition. In this recipe, we will create three basic functions and look at how to compose a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to build a function compositions for data processing. We'll do it in the `function_composition.py `file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we have created three basic functions and have learned how to compose a pipeline. To do this, the `reduce()` function was used. This function accepts a function and a sequence and returns a single value. \n",
    "\n",
    "The reduce () function calculates the return value, as follows:\n",
    "\n",
    "* To start, the function calculates the result by using the first two elements of the sequence.\n",
    "* Next, the function uses the result obtained in the previous step and the next value in the sequence.\n",
    "* This process is repeated until the end of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The three basic functions used at the beginning of the recipe make use of the `map()` function. This function is used to apply a function on all the elements of a specific value. As a result, a map object is returned; this object is an iterator, so we can iterate over its elements. To print this object, we have converted the map object to sequence objects as a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building machine learning pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` library is used to build machine learning pipelines. When we define the functions, the library will build a composed object that makes the data go through the entire pipeline. This pipeline can include functions, such as preprocessing, feature selection, supervised learning, and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will be building a pipeline to take the input feature vector, select the top k features, and then classify them using a random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to build machine learning pipelines in the `pipeline.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The advantage of selecting the $k$ best features is that **we will be able to work with low-dimensional data**. This is helpful in **reducing the computational complexity**. The way in which we select the $k$ best features is based on univariate feature selection. This performs univariate statistical tests and then extracts the top performing features from the feature vector. Univariate statistical tests refer to analysis techniques where a single variable is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Once these tests are performed, each feature in the feature vector is assigned a score. Based on these scores, we select the top $k$ features. We do this as a preprocessing step in our classifier pipeline. Once we extract the top k features, a $k$-dimensional feature vector is formed, and we use it as the input training data for the random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Finding the nearest neighbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbors model refers to a general class of algorithms that aim to make a decision based on the number of nearest neighbors in the training dataset. The nearest neighbors method consists of finding a predefined number of training samples that are close to the distance from the new point and predicting the label. The number of samples can be user defined, consistent, or differ from each other – it depends on the local density of points. The distance can be calculated with any metric measure – the standard Euclidean distance is the most common choice. Neighbor-based methods simply remember all training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will find the nearest neighbors using a series of points on a Cartesian plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to find the nearest neighbors in the `knn.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we looked for the nearest neighbors by using a series of points on a Cartesian plane. To do this, the space is partitioned into regions based on the positions and characteristics of the training objects. This can be considered as the training set for the algorithm even if it is not explicitly required by the initial conditions. To calculate the distance, the objects are represented through position vectors in a multidimensional space. Finally, a point is assigned to a class if it is the most frequent of the k examples closest to the object under examination. The proximity is measured by the distance between points. Neighbors are taken from a set of objects for which the correct classification is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To build the nearest neighbors model, the **BallTree** algorithm was used. **BallTree is a data structure that organizes points in a multidimensional space**. The algorithm gets its name because it **partitions datapoints into a nested set of hyperspheres, known as balls. It's useful for a number of applications, most notably, the nearest neighbor search.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a $k$-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbors algorithm is an algorithm that uses k-nearest neighbors in the training dataset to find the category of an unknown object. When we want to find the class that an unknown point belongs to, we find the k-nearest neighbors and take a majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will create a k-nearest neighbors classifier starting from the input data that contains a series of points arranged on a Cartesian plane that shows a grouping within three areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to build a k-nearest neighbors classifier in the `nn_classification.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The $k$-nearest neighbors classifier stores all the available datapoints and classifies new datapoints based on a similarity metric. This similarity metric usually appears in the form of a distance function. This algorithm is a nonparametric technique, which means that it doesn't need to find out any underlying parameters before formulation. All we need to do is select a value of k that works for us.\n",
    "\n",
    "Once we find out the $k$-nearest neighbors classifier, we take a majority vote. A new datapoint is classified by this majority vote of the $k$-nearest neighbors classifier. This datapoint is assigned to the class that is most common among its $k$-nearest neighbors. If we set the value of $k$ to 1, then this simply becomes a case of a nearest neighbor classifier where we just assign the datapoint to the class of its nearest neighbor in the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The $k$-nearest neighbor algorithm is based on the concept of classifying an un$k$nown sample by considering the class of $k$ samples closest to the training set. The new sample will be assigned to the class that most of the $k$ nearest samples belong to. The choice of $k$ is, therefore, very important for the sample to be assigned to the correct class. If $k$ is too small, the classification may be sensitive to noise; if $k$ is too large, the classification may be computationally expensive, and the neighborhood may include samples belonging to other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a $k$-nearest neighbors regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned how to use the k-nearest neighbors algorithm to build a classifier. The good thing  is that we can also use this algorithm as a regressor. The object's output is represented by it's property value, which is the average of the values of its $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will see how to use to use the $k$-nearest neighbors algorithm to build a regressor. We will work in the `nn_regressor.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.masakhane.io/\n",
    "\n",
    "https://traductorlab-bf.github.io/projet-futurs/projet-data-Dagaare-fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The goal of a regressor is to predict continuous valued outputs. We don't have a fixed number of output categories in this case. We just have a set of real-valued output values, and we want our regressor to predict the output values for unknown datapoints. In this case, we used a sinc function to demonstrate the k-nearest neighbors regressor. This is also referred to as the cardinal sine function. A sinc function is defined by the following equation:\n",
    "$sinc(x) = \\{^{sin(x)/x when x \\not 0}_{1 when w = 0}$\n",
    "\n",
    "\n",
    "When $x$ is $0$, $sin(x)/x$ takes the indeterminate form of $0/0$. Hence, we have to compute the limit of this function as $x$ tends to be $0$. We used a set of values for training, and we defined a denser grid for testing. As you can see in the preceding diagram, the output curve is close to the training outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The main advantages of this method are that it does not require learning or the construction of a model; it can adapt its decision boundaries in an arbitrary way, producing a representation of the most flexible model; and it also guarantees the possibility of increasing the training set. However, this algorithm also has many drawbacks, including being susceptible to data noise, being sensitive to the presence of irrelevant features, and requiring a similarity measure to evaluate proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the euclidean distance score\n",
    "\n",
    "Now that we have sufficient background in machine learning pipelines and the nearest neighbors classifier, let's start the discussion on recommendation engines. In order to build a recommendation engine, we need to define a similarity metric so that we can find users in the database who are similar to a given user. The Euclidean distance score is one such metric that we can use to compute the distance between datapoints. We will shift the discussion toward movie recommendation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will see how to compute the Euclidean score between two users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to compute the Euclidean distance score. We'll use the  `euclidian_score.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In most cases, the distance used in the nearest neighbors algorithm is defined as the Euclidean distance between two\n",
    "points, calculated according to the following formula:\n",
    "\n",
    "$distance = \\sqrt{\\sum^{n}_{i = 0}(x_i - y_i)^2}$\n",
    "\n",
    "On a bidimensional plane, the Euclidean distance represents the minimum distance between two points, hence the straight line connecting two points. This distance is calculated as the square root of the sum of the squared difference between the elements of two vectors, as indicated in the previous formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "There are other types of metrics for calculating distances. All of these types generally try to avoid the square roots, since they are expensive in computational terms, and are the source of several errors. Metrics include **Minkowski**, **Manhattan**, and the **cosine** distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Pearson correlation score\n",
    "The Euclidean distance score is a good metric, but it **has some shortcomings**. Hence, **the Pearson correlation score is frequently used in recommendation engines**. The Pearson correlation score between two statistical variables `is an index that expresses a possible linear relation between them. It measures the tendency of two numerical variables to vary simultaneously`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will see how to compute the Pearson correlation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to compute the Pearsn correlation score. We'll use the `pearson_score.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The $r$ correlation coefficient of Pearson measures the correlation between variables at intervals or equivalent ratios. It is given by the sum of the products of the standardized scores of the two variables $(z_x * z_y)$ divided by the number of subjects (or observations), as follows:\n",
    "\n",
    "$r = \\frac{\\sum{z_x * z_y}}{N}$\n",
    "\n",
    "This coefficient can assume values ranging between -1.00 (between the two variables, there is a perfect negative correlation) and + 1.00 (between the two variables, there is a perfect positive correlation). A correlation of 0 indicates that there is no relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "It is necessary to remember that Pearson's formula is related to a linear relationship, and therefore, all the different forms of relationship can produce anomalous results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding similar users in the dataset\n",
    "One of the most important tasks in building a recommendation engine is finding users who are similar. This is useful in creating the recommendations that will be provided to these users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will see how to build a model to find users who are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to build a model to find users who are similar. We'll use the `find_similar_users.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There's more\n",
    "To calculate the Pearson correlation score, the pearson_score() function was used. This function was defined in the previous Computing the Pearson correlation score recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating movie recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate movie recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use all the functionality that we built in the previous recipes to build a movie recommendation engine. Let's take a look at how to build it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to generate movie recommendations. We'll be working in the `movie_recommandations.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we have built a movie recommendation engine. To generate recommendations for a given user, the following steps are performed:\n",
    "\n",
    "1. First, we check whether the user is present in the database\n",
    "2. Then, we calculate the Person correlation score\n",
    "3. We then create the normalized list\n",
    "4. Then, we sort this list in decreasing order based on the first column\n",
    "5. Finally, we extract the recommended movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To build a movie recommendation engine, the `pearson_score()` function was used. This function was defined in the previous *Computing the Pearson correlation score* recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing ranking algorithms\n",
    "Learning to rank (LTR) **is a method that is used in the construction of classification models for information retrieval systems**. The training data consists of lists of articles with an induced partial order that gives a numerical or ordinal score, or a binary judgment for each article. The purpose of the model is to order the elements into new lists according to the scores that take into account the judgments obtained from the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ready\n",
    "In this recipe, we will use the **pyltr** package, which is a Python LTR toolkit with ranking models, evaluation metrics, and data-wrangling helpers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's take a look at how to implement ranking algorithms, we'll be working in the `LambdaMARTModel.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "LambdaMART is the enhanced tree version of LambdaRank, which is, in turn, based on RankNet. RankNet, LambdaRank, and LambdaMART are algorithms that are used to solve classification problems in many contexts. RankNet, LambdaRank, and LambdaMART have been developed by Chris Burges and his group at Microsoft Research. RankNet was the first one to be developed, followed by LambdaRank, and then LambdaMART.\n",
    "\n",
    "RankNet is based on the use of neural networks, but the underlying model is not limited to neural networks alone. The cost function for RankNet aims to minimize the number of reversals in the ranking. RankNet optimizes the cost function using the stochastic gradient descent.\n",
    "\n",
    "The researchers found that during the RankNet training procedure, the costs are not required, only the gradients (λ) of the cost compared to the model score. You can think of these gradients as small arrows attached to each document in the classified list, indicating the direction in which we could move those documents. LambdaRank is based on this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Finally, LambdaMART combines the methods contained in LambdaRank and those present in **multiple regression additive trees (MART)**. While MART uses decision trees with enhanced gradient for forecasting, LambdaMART uses enhanced gradient decision trees using a cost function derived from LambdaRank to solve a ranking task. LambdaMART proved to be more efficient than LambdaRank and the original RankNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a filtering model using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering refers to a class of tools and mechanisms that allow the retrieval of predictive information regarding the interests of a given set of users starting from a large and yet undifferentiated mass of knowledge. Collaborative filtering is widely used in the context of recommendation systems. A well-known category of collaborative algorithms is matrix factorization.\n",
    "\n",
    "The fundamental assumption behind the concept of collaborative filtering is that every single user who has shown a certain set of preferences will continue to show them in the future. A popular example of collaborative filtering can be a system of suggested movies starting from a set of basic knowledge of the tastes and preferences of a given user. It should be noted that although this information is referring to a single user, they derive this from the knowledge that has been processed throughout the whole system of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will see how to build a collaborative filtering model for personalized recommendations using TensorFlow. We will use the MovieLens 1M dataset, which contains $1$ million ratings from approximately $6,000$ users for approximately $4,000$ movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to build a filtering model using Tensorflow. We'll be working in the `tensorFilter.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The collaborative filter approach focuses on finding users who have made similar judgments to the same objects, thus creating a link between users, to whom will be suggested objects that one of the two has reviewed in a positive way, or simply with which they have interacted. In this way, we look for associations between users, and no longer between objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The user item matrix represents a user's preferences for an object, but, if read by columns, highlights who a certain movie was liked or disliked by. In this way, you can see how a similarity between two objects can also be expressed without the object matrix, simply by observing that the films that are liked by the same people are probably similar in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5052bdfd4928f465c492d21ff3d7daf709b24acb81685a4e868a1ee9dbe33934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
