{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our brain is really good at identifying and recognizing things. We want machines to be able to do the same. A neural network is a framework that is modeled after the human brain to simulate our learning processes. Neural networks are designed to learn from data and recognize the underlying patterns. As with all learning algorithms, neural networks deal with numbers. Therefore, if we want to achieve any real-world task involving images, text, sensors, and so on, we have to convert them into a numerical format before we feed them into a neural network. We can use a neural network for classification, clustering, generation, and many other related tasks.\n",
    "\n",
    "A neural network consists of layers of neurons. These neurons are modeled after the biological neurons in the human brain. Each layer is basically a set of independent neurons that are connected to the neurons on adjacent layers. The input layer corresponds to the input data that we provide, and the output layer consists of the output that we desire. All the layers in between are called hidden layers. If we design a neural network with more hidden layers, then we give it more freedom to train itself with higher accuracy.\n",
    "\n",
    "Let's say that we want the neural network to classify data, based on our needs. For a neural network to work accordingly, we need to provide labeled training data. The neural network will then train itself by optimizing the cost function. This cost function is the error between actual labels and the predicted labels from the neural network. We keep iterating until the error goes below a certain threshold.\n",
    "\n",
    "What exactly are deep neural networks? Deep neural networks are neural networks that consist of many hidden layers. In general, this falls under the realm of deep learning. This is a field that is dedicated to the study of these neural networks, composed of multiple layers that are used across many verticals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start our neural network adventure with a perceptron. A perceptron is a single neuron that performs all the computations. It is a very simple model, but it forms the basis of building up complex neural networks. The following is what it looks like:\n",
    "\n",
    "![](perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neuron combines inputs using different weights, and it then adds a bias value to compute the output. It's a simple linear equation relating input values to the output of the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use a library called <t style=\"color: red\">neurolab</t> to define a perceptron with two inputs Before you proceed, make sure that you install it\n",
    "\n",
    "We will be working in the `perceptron.py` file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "we used a single neuron that performs all the computations. To train the **perceptron**, the following parameters are set. The number of epochs specifies the number of complete passes through our training dataset. The **show** parameter specifies how frequently we want to display the progress. The **lr** parameter specifies the learning rate of the **perceptron**. It is the step size for when the algorithm searches through the parameter space. If this is large, then the algorithm may move faster, but it might miss the optimum value. If this is small, then the algorithm will hit the optimum value, but it will be slow. So, it's a trade-off; hence, we choose a value of $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "We can understand a perceptron concept as anything that takes multiple inputs and produces one output. This is the simplest form of a neural network. The perceptron concept was suggested by Frank Rosenblatt in 1958 as an object with an input and output layer and a learning rule targeted at minimizing errors. This learning function called **error backpropagation** changes connective weights (synapses) relying on the actual output of the network, with respect to a given input, as the difference between the actual output and the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a single layer neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we learned how to create a perceptron; now let's create a single layer neural network. A single layer neural network consists of multiple neurons in a single layer. Overall, we will have an input layer, a hidden layer, and an output layer, as shown in the following diagram\n",
    "![](1_layer_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "we will learn how to create a single layer neural network using the neurolab library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a single layer neural network, we will be working in the `single_layer.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "A single layer neural network has the following architecture: the inputs form the input layer, the middle layer that performs the processing is called the hidden layer, and the outputs form the output layer. The hidden layer can convert the input to the desired output. Understanding the hidden layer requires knowledge of weights, bias, and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "**Weights are vital to convert an input so it impacts the output**; they are numerical parameters that monitor how all of the neurons affect the others. The related concept resembles the slope in linear regression, where a weight is multiplied to the input to add up and form the output. \n",
    "\n",
    "**Bias is similar to the intercept added to a linear equation**. It is also an **additional parameter that is used to regulate the output along with the weighted sum** of the inputs to the neuron.\n",
    "\n",
    "**An activation function is a mathematical function that converts the input to an output and determines the total signal a neuron receives**. **Without activation functions, neural networks would behave like linear functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a deep neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build a deep neural network. A deep neural network consists of an input layer, many hidden layers, and an output layer. This looks like the following:\n",
    "\n",
    "![](deep_nn.png)\n",
    "\n",
    "The preceding diagram depicts a multilayer neural network with one input layer, one hidden layer, and one output layer. In a deep neural network, there are many hidden layers between the input and output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "we will build a deep neural network. Deep learning forms an advanced neural network with numerous hidden layers. Deep learning is a vast subject and is an important concept in building AI. In this recipe, we will use generated training data and define a multilayer neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a deep neural network, we will be working in the `deep_neural_network.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "we will use generated training data to train a multilayer deep neural network with two hidden layers. To train the model, the gradient descent algorithm was used. **Gradient descent is an iterative approach used for error correction in any learning model**. A gradient descent approach is the process of iterating u**pdating weights and biases with the error times derivative of the activation function (backpropagation)**. In this approach, the steepest descent step size is substituted by a similar size from the previous step. Gradient is the slope of the curve, as it is the derivative of the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The objective of finding the gradient descent at every step is **to find the global cost minimum**, where **the error is the lowest**. And this is where the model has a good fit for the data, and predictions are more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a vector quantizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use neural networks for vector quantization as well. **Vector quantization** is the $N$-dimensional version of rounding-off. This is very commonly used accross multiple areas in computer vision, NLP and machine learning in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In previous recipes, we already addressed vector quantization concepts: *Compressing ad image using vector quantization* and *creating feeatures using visual codebook and vector quantization*. In this recipe, we will define a neural network with 2 layers-$10$ neurons in input layer and 4 neurons in the output layer. Then we will use this network to divide the space into 4 regions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to create a vector quantizer. We'll be working in the `vector_quantizer.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "We defined a neural network with two layers: 10 neurons in the input layer and 4 neurons in the output layer. This neural network was first trained and then used to divide the space into four regions. Each region corresponds to a bucket in the list of vector-quantized regions in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Theres's more\n",
    "**Vector quantization** is based on the division of a large set of points (vectors) into groups that show the same number of points closer to them. Each group is identified by its centroid point, as is the case with most clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a recurrent neural network for sequential data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are really good at analyzing sequential and time-series data. A **recurrent neural network (RNN)** is a <t style=\"color: green\">neural model in which a bidirectional flow of information is present</t>. In other words, <t style=\"color: green\">while the propagation of signals in feedforward networks takes place only in a continuous manner, going from inputs to outputs, recurrent networks are different</t>. In them, <t style=\"color: green\">this propagation can also occur from a neural layer following a previous one, or between neurons belonging to the same layer, and even between a neuron and itself.</t>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "When we deal with sequential and time-series data, we cannot just extend generic models. The temportal dependencies in hte data are really important, and we need to account for this in our models. Let's build a rnn using neurolab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a recurrent neural network for sequential data analysis, we'll be coding in the `reccurent_network.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    " In this recipe, first, we created an artificial signal with waveform characteristics, that is, a curve showing the shape of a wave at a given time. Then we built a recurrent neural network to see whether the network could predict a waveform of random length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Recurrent networks are distinguished from feedforward networks thanks to the feedback loop linked to their past decisions, thus accepting their output momentarily as inputs. This feature can be emphasized by saying that recurrent networks have memory. There is information in the sequence, and it is used perform the tasks that feedforward networks cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the characters in an OCR database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at how to use neural networks to perform **optical character recognition (OCR)**. This refers to the *process of identifying handwritten characters in images*. We have always been particularly sensitive to the problem of the automatic recognition of writing in order to achieve a simpler interaction between humans and machines. Especially in the last few years, this problem has been subject to interesting developments and more and more efficient solutions thanks to a very strong economic interest and an ever-greater capacity to process data of modern computers. In particular, some countries, such as Japan, and Asian countries in general, are investing heavily, in terms of research and financial resources, to make state-of-the-art OCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will display the handwritten digits contained in a dataset. We will use the dataset available at [this link](http://ai.stanford.edu/~btaskar/ocr). The default file name after downloading is `letter.data`. To start with, let's see how to interact with the data and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to visualize the characters in an OCR database. We'll be working in the `visualize_characters.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we showed the handwritten figures contained in a dataset. To do this, the following tasks are performed:\n",
    "\n",
    "* Load input data\n",
    "* Define visualization parameters\n",
    "* Loop until you encounter the Escape key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Approaches to the OCR problem are basically of two types: one is based on pattern matching or on the comparison of a model and the other is based on structural analysis. Often, these two techniques are used in combination, and provide remarkable results in terms of recognition and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an optical character recognizer using neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will build a neural network-based OCR system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build an optical character recognizer using neural networks. We'll be working in the `ocr.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we used a neural network to recognize the handwritten digits. To do this, the following tasks are performed:\n",
    "\n",
    "* Loading and manipulating input data\n",
    "* Creating the dataset\n",
    "* Converting data and labels into NumPy arrays\n",
    "* Extracting the number of dimensions\n",
    "* Creating and training the neural network\n",
    "* Phredicting the output for test inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The term **handwriting recognition (HWR)** refers to the ability of a computer to receive and interpret as text intelligible handwritten input from sources such as paper documents, photographs, and touchscreens. Written text can be detected on a piece of paper via optical scanning (OCR) or **intelligent word recognition**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing optimization algorithms in ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have built several neural networks and obtained satisfactory overall performances. We have evaluated the model's performance using the loss function, which is a mathematical way to measure how wrong our predictions are. To improve the performance of a model based on neural networks, during the training process, **weights are modified to try to minimize the loss function and make our predictions as correct as possible**. To do this, optimizers are used: they are algorithms that regulate the parameters of the model, updating it in relation to what is returned by the loss function. In practice, optimizers shape the model in its most accurate form possible by overcoming weights: The loss function tells the optimizer when it is moving in the right or wrong direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will build a neural network using the Keras library and improve the performance of the model by adopting several optimizers. To do this, the iris dataset will be used. I'm referring to the **Iris flower dataset**, a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper: The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to implement optimization algorithms in ANN. We'll be coding in the `IrisClassifier.py`file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "As we said in the *Building a deep neural network* recipe, gradient descent is an iterative approach used for error correction in any learning model. Gradient descent approach is the process of iterating the update of weights and biases with the error times derivative of the activation function (backpropagation). In this approach, the steepest descent step size is substituted by a similar size from the previous step. The gradient is the slope of the curve, as it is the derivative of the activation function. The SGD optimizer is based on this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Theres's more\n",
    "Optimization problems are usually so complex that it is not possible to determine a solution analytically. Complexity is determined primarily by the number of variables and constraints, which define the size of the problem, and then by the possible presence of non-linear functions. An analytical solution is only possible in the case of a few variables and extremely simple functions. In practice, to solve an optimization problem, it is necessary to resort to an iterative algorithm, that is, to a calculation program that, given a current approximation of the solution, determines, with an appropriate sequence of operations, a new approximation. Starting from an initial approximation, a succession is thus determined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5052bdfd4928f465c492d21ff3d7daf709b24acb81685a4e868a1ee9dbe33934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
