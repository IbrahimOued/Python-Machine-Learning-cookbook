{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text analysis and **natural language processing (NLP)** are an integral part of modern artificial intelligence systems. Computers are good at understanding rigidly structured data with limited variety. However, when we deal with unstructured, free-form text, things begin to get difficult. Developing NLP applications is challenging because computers have a hard time understanding the underlying concepts. There are also many subtle variations to the way that we communicate things. These can be in the form of dialects, context, slang, and so on.\n",
    "\n",
    "In order to solve this problem, NLP applications are developed based on machine learning. These algorithms detect patterns in text data so that we can extract insights from them. Artificial intelligence companies make heavy use of NLP and text analysis in order to deliver relevant results. Some of the most common applications of NLP include search engines, sentiment analysis, topic modeling, part-of-speech tagging, and entity recognition. The goal of NLP is to develop a set of algorithms so that we can interact with computers in plain English. If we can achieve this, then we won't need programming languages to instruct computers on what they should do. In this chapter, we will look at a few recipes that focus on text analysis and how we can extract meaningful information from text data.\n",
    "\n",
    "We will use a Python package called **Natural Language Toolkit (NLTK)** heavily in this chapter. Make sure that you install this before you proceed:\n",
    "\n",
    "* You can find the installation steps at http://www.nltk.org/install.html.\n",
    "* You will also need to install **NLTK Data**, which contains many corpora and trained models. This is an integral part of text analysis! You can find the installation steps at http://www.nltk.org/data.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data using tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of dividing text into a set of meaningful pieces. These pieces are called tokens. For example, we can divide a chunk of text into words, or we can divide it into sentences. Depending on the task at hand, we can define our own conditions to divide the input text into meaningful tokens. Let's take a look at how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Tokenization is the first step in the computational analysis of the text and involves dividing the sequences of characters into minimal units of analysis called **tokens**. Tokens include various categories of text parts (words, punctuation, numbers, and so on), and can also be complex units (such as dates). In this recipe, we will illustrate how to divide a complex sentence into many tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to preprocess data using tokenization\n",
    "We'll use the `tokenizer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "n this recipe, we illustrated how to divide a complex sentence into many tokens. To do this, three methods of the nltk.tokenize package were usedâ€”sent_tokenize, word_tokenize, and WordPunctTokenizer:\n",
    "\n",
    "* `sent_tokenize` returns a sentence-tokenized copy of text, using NLTK's recommended sentence tokenizer.\n",
    "* `word_tokenize` tokenizes a string to split off punctuation other than periods.\n",
    "* `WordPunctTokenizer` tokenizes a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \\w+|[^\\w\\s]+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Tokenization is a procedure that, depending on the language that is analyzed, can be an extremely complex task. In English, for example, we could be content to consider taking sequences of characters that do not have spaces and the various punctuation marks. Languages such as Japanese or Chinese, in which words are not separated by spaces but the union of different symbols, can completely change the meaning, and the task is much more complex. But in general, even in languages with words separated by spaces, precise criteria must be defined, as punctuation is often ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with a text document, we encounter different forms of words. Consider the word **play**. This word can appear in various forms, such as *play, plays, player, playing*, and so on. These are basically families of words with similar meanings. During text analysis, it's useful to extract the base forms of these words. This will help us to extract some statistics to analyze the overall text. The goal of **stemming** is to reduce these different forms into a common base form. This uses a heuristic process to cut off the ends of words in order to extract the base form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the `nltk.stem()` package that offers a preprocessing interface for removing morphological affixes from words. Different stemmers are available for different languages. For the english language, we will use `PorterStemmer`, `LancasterStemmer` and `SnowballStemmer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's loog at how to stem text data\n",
    "We'll use the `stemmer.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "All three stemming algorithms basically aim at achieving the same thing. The difference between the three stemming algorithms is basically the level of strictness with which they operate. If you observe the output, you will see that the LANCASTER stemmer is stricter than the other two stemmers. The PORTER stemmer is the least in terms of strictness, and the LANCASTER is the strictest. The stemmed words that we get from the LANCASTER stemmer tend to get confusing and obfuscated. The algorithm is really fast, but it will reduce the words a lot. So, a good rule of thumb is to use the SNOWBALL stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Stemming is the process of reducing the inflected form of a word to its root form, called the **stem**. The stem doesn't necessarily correspond to the morphological root (lemma) of the word: it's normally sufficient that the related words are mapped to the same stem, even if the latter isn't a valid root for the word. The creation of a stemming algorithm has been a prevalent issue in computer science. The stemming process is applied in search engines for query expansion, and in other natural language processing problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting text to its base form using lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of **lemmatization is also to reduce words to their base forms**, but this is a more structured approach. In the previous recipe, you saw that the base words that we obtained using stemmers don't really make sense. For example, the word *wolves* was reduced to *wolv*, which is not a real word. **Lemmatization solves this problem by doing things with a vocabulary and morphological analysis of words**. It **removes inflectional word endings, such as -ing or -ed, and returns the base form of a word**. This base form is known as the **lemma**. If you lemmatize the word wolves, you will get wolf as the output. The output depends on whether the token is a verb or a noun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the nltk.stem package to reducing a word's inflected form to its canonical form, called a **lemma**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to convert text to its base form using lemmatization:\n",
    "We'll use the `lemmatizer.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Lemmatization **is the process of reducing a word's inflected form to its canonical form, called a lemma**. In the processing of natural language, lemmatization is the **algorithmic process that automatically determines the word of a given word**. The process may involve other language processing activities, such as morphological and grammatical analysis. In many languages, words appear in different inflected forms. The combination of the canonical form with its part of speech is called the lexeme of the word. A **`lexeme`** is, in structural lexicology, **the minimum unit that constitutes the lexicon of a language**. Hence, **every lexicon of a language may correspond to its registration in a dictionary in the form of a lemma**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "In NLTK, for lemmatization, WordNet is available, but this resource is limited to the English language. It's a large lexical database of the English language. In this package, names, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (**synsets**), each of which expresses a distinct concept. The synsets are interconnected by means of semantic and lexical conceptual relationships. The resulting network of significantly related words and concepts can be navigated with the browser. WordNet groups words according to their meanings, connecting not only word forms (strings of letters) but specific words. Hence, the words that are in close proximity to each other in the network are semantically disambiguated. In addition, WordNet labels the semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing text using chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking** refers to **dividing the input text into pieces**, which are based on any random condition. This is different from tokenization in the sense that there are no constraints, and the chunks do not need to be meaningful at all. This is used very frequently during text analysis. While dealing with large text documents, it's better to do it in chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look how to divide text by using chunking\n",
    "We'll work with the `chunking.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Chunking (also called **shallow parsing**) is the analysis of a proposition, which is formed in a simple form by a subject and a predicate. The subject is typically a noun phrase, while the predicate is a verbal phrase formed by a verb with zero or more complements and adverbs. A chunk is made up of one or more adjacent tokens.\n",
    "\n",
    "There are numerous approaches to the problem of chunking. For example, in the assigned task, a chunk is represented as a group of words delimited by square brackets, for which a tag representing the type of chunk is indicated. The dataset that was used was derived from a given corpora by taking the part related to journal articles and extracting chunks of information from the syntactic trees of the corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The Brown University Standard Corpus of Present-Day American English (or simply, the brown corpus) is a corpus that was compiled in the 1960s by Henry Kucera and W. Nelson Francis at Brown University, Providence, Rhode Island. It contains 500 text extracts in English, obtained from works published in the United States of America in 1961, for a total of about one million words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a bad-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to dealing with text documents that consist of millions of words, **converting them into numerical representations is necessary**. The reason for this is to **make them usable for machine learning algorithms**. These algorithms need numerical data so that they can analyze them and output meaningful information. This is where the **bag-of-words** approach comes into the picture. This is basically a model that **learns a vocabulary from all of the words in all the documents**. It models each document by building a histogram of all of the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will build a bag-of-words model to extract a document term matrix, using the `sklearn.feature_extraction.text` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to build a bag-of-words model, as follows:\n",
    "We'll use the `bag_of_words.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Consider the following sentences:\n",
    "\n",
    "* Sentence 1: The brown dog is running.\n",
    "* Sentence 2: The black dog is in the black room.\n",
    "* Sentence 3: Running in the room is forbidden.\n",
    "\n",
    "If you consider all three of these sentences, you will have the following nine unique words:\n",
    "\n",
    "* the\n",
    "* brown\n",
    "* dog\n",
    "* is\n",
    "* running\n",
    "* black\n",
    "* in\n",
    "* room\n",
    "* forbidden\n",
    "\n",
    "Now, let's convert each sentence into a histogram, using the count of words in each sentence. Each feature vector will be nine-dimensional, because we have nine unique words:\n",
    "\n",
    "* Sentence 1: [1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "* Sentence 2: [2, 0, 1, 1, 0, 2, 1, 1, 0]\n",
    "* Sentence 3: [0, 0, 0, 1, 1, 0, 1, 1, 1]\n",
    "Once we have extracted these feature vectors, we can use machine learning algorithms to analyze them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The bag-of-words model is a method that's used in information retrieval and in the processing of the natural language in order to represent documents by ignoring the word order. In this model, each document is considered to contain words, similar to a stock exchange; this allows for the management of these words based on lists, where each stock contains certain words from a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of text classification is to sort text documents into different classes. This is a vital analysis technique in NLP. We will use a technique that is based on a statistic called **tf-idf**, which stands for **term frequency-inverse document frequency**. This is an analysis tool that helps us to understand how important a word is to a document in a set of documents. This serves as a feature vector that's used to categorize documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the term frequency-inverse document frequency method to evaluate the importance of a word for a document in a collection or a corpus, and to build a text classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to build a text classifier\n",
    "We'll use the `tfidf.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The tf-idf technique is used frequently in information retrieval. The goal is to understand the importance of each word within a document. We want to identify words that occur many times in a document. At the same time, common words such as **is** and **be** don't really reflect the nature of the content. So, we need to extract the words that are true indicators. The importance of each word increases as the count increases. At the same time, as it appears a lot, the frequency of this word increases, too. These two things tend to balance each other out. We extract the term counts from each sentence. Once we have converted this to a feature vector, we can train the classifier to categorize these sentences.\n",
    "\n",
    "**The term frequency (TF) measures how frequently a word occurs in a given document**. As multiple documents differ in length, **the numbers in the histogram tend to vary a lot**. So, we need to normalize this so that it becomes a level playing field. **To achieve normalization, we can divide the term-frequency by the total number of words in a given document**. The inverse document frequency (IDF) **measures the importance of a given word**. When we compute the TF, all words are considered to be equally important. To counterbalance the frequencies of commonly occurring words, we need to weigh them down and scale up the rare ones. We need to calculate the ratio of the number of documents with the given word and divide it by the total number of documents. The IDF is calculated by taking the negative algorithm of this ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Simple words, such as **is** or **the**, tend to appear a lot in various documents. However, this doesn't mean that we can characterize the document based on these words. At the same time, if a word appears a single time, that is not useful, either. So, we look for words that appear a number of times, but not so much that they become noisy. This is formulated in the tf-idf technique and is used to classify documents. Search engines frequently use this tool to order search results by relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the gender of a name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the gender of a name is an interesting task in NLP. We will use the **heuristic** that the last few characters in a name is its defining characteristic. For example, if the name ends with ***la***, it's most likely a female name, such as Angela or Layla. On the other hand, if the name ends with ***im***, it's most likely a male name, such as Tim or Jim. As we aren't sure of the exact number of characters to use, we will experiment with this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the names corpora to extract labeled names, and then we will classify the gender based on the final part of the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to identify gender\n",
    "We'll use the `gender_identification.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we used the names corpus to extract labeled names, and then we classified the gender based on the final part of the name. A Naive Bayes classifier is a supervised learning classifier that uses Bayes' theorem to build the model. This topic was addressed in the Building a Naive Bayes classifier recipe in Chapter 2, Constructing a Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The Bayesian classifier is called naive because it ingenuously assumes that the presence or absence of a particular characteristic in a given class of interest is not related to the presence or absence of other characteristics, greatly simplifying the calculation. Let's go ahead and build a Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the sentiment of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis** is one of the most popular applications of NLP. Sentiment analysis refers to the process of determining whether a given piece of text is positive or negative. In some variations, we consider neutral as a third option. This technique is commonly used to discover how people feel about a particular topic. This is used to analyze the sentiments of users in various forms, such as marketing campaigns, social media, e-commerce, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will analyze the sentiment of a sentence by using a Naive Bayes classifier, starting with the data contained in hte movie_reviews corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to analyze the sentiment of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "We used **NLTK**'s Naive Bayes classifier for our task here. In the feature extractor function, we basically extracted all the unique words. However, the **NLTK** classifier needs the data to be arranged in the form of a dictionary. Hence, we arranged it in such a way that the NLTK classifier object can ingest it. Once we divided the data into training and testing datasets, we trained the classifier to categorize the sentences into positive and negative ones. \n",
    "\n",
    "If you look at the top informative words, you can see that we have words such as outstanding to indicate positive reviews and words such as insulting to indicate negative reviews. This is interesting information, because it tells us what words are being used to indicate strong reactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The term sentiment analysis refers to the use of NLP techniques, text analysis, and computational linguistics to find information in written or spoken text sources. If this subjective information is taken from large amounts of data, and therefore from the opinions of large groups of people, sentiment analysis can also be called **opinion mining**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying patterns in text using topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic modeling** refers to the process of identifying hidden patterns in text data. The goal is to uncover a hidden thematic structure in a collection of documents. This will help us to organize our documents in a better way, so that we can use them for analysis. This is an active area of research in NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting ready**\n",
    "In this recipe, we will use a librairy called `gensim` to identify patterns in text, using topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to identify patterns in text by using topic modelling.\n",
    "We'll use the `topic_modeling.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "**Topic modeling** works by identifying the important words or themes in a document. These words tend to determine what the topic is about. We use a regular expression tokenizer, because we just want the words, without any punctuation or other kinds of tokens. Hence, we use this to extract the tokens. The stop word removal is another important step, because this helps us to eliminate the noise caused by words such as **is** or **the**. After that, we need to stem the words to get to their base forms. This entire thing is packaged as a preprocessing block in text analysis tools. That is what we are doing here, as well!\n",
    "\n",
    "We use a technique called LDA to model the topics. LDA basically represents the documents as a mixture of different topics that tend to spit out words. These words are spat out with certain probabilities. The goal is to find these topics! This is a generative model that tries to find the set of topics that are responsible for the generation of the given set of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "As you can see from the output, we have words such as talent and train to characterize the sports topic, whereas we have encrypt to characterize the cryptography topic. We are working with a really small text file, which is why some words might seem less relevant. Obviously, the accuracy will improve if you work with a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of speech tagging with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parts-of-speech tagging (PoS tagging)** is the process of labeling the words that correspond to particular lexical categories. The common linguistic categories include nouns, verbs, adjectives, articles, pronouns, adverbs, conjunctions, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe; we'll use `spacy` to perform POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to perform PoS tagging using spacy\n",
    "We'll use the `PosTagging.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "PoS tagging involves **assigning a tag to each word of a document/corpus**. The choice of the tagset to use depends on the language. The input is a string of words and the tagset to be used, and the output is the association of the best tag with each word. There may be multiple tags compatible with a word (**ambiguity**). The task of the PoS tagger is to solve these ambiguities by choosing the most appropriate tags, based on the context in which the word is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To perform a PoS tagging, we used the `spacy` library. This library extracts linguistic features, such as PoS tags, dependency labels, and named entities, customizing the tokenizer and working with the rule-based matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec using gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embedding** allows us to memorize both the semantic and syntactic information of words, starting with an unknown corpus and constructing a vector space in which the vectors of words are closer if the words occur in the same linguistic contexts, that is, if they are recognized as semantically similar. Word2Vec is a set of templates that are used to produce word embedding; the package was originally created in C by Tomas Mikolov, and was then implemented in Python and Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the `gensim` library to build a Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to perform word embedding by using gensim\n",
    "We will be using the `GensimWord2Vec.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "**Word2Vec is a simple two-layer artificial neural network** that was designed to process natural language; the algorithm **requires a corpus in the input and returns a set of vectors that represent the semantic distributions of words in the text**. For each word contained in the corpus, in a univocal way, a vector is constructed in order to **represent it as a point in the created multidimensional space**. In this space, the words will be closer if they are recognized as semantically more similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "In this recipe, we used the Australian National Corpus (*abc*), a great collection of language data, both text-based and digital. To use this corpus, you must download it with the following code:\n",
    "`import nltk`\n",
    "`nltk.download('abc')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow learning for spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spamming** means sending large amounts of unwanted messages (usually commercial). It can be implemented through any medium, but the most commonly used are email and SMS. The main purpose of spamming is advertising, from the most common commercial offers to proposals for the sale of illegal material, such as pirated software and drugs without a prescription, and from questionable financial projects to genuine attempts at fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use a logistic regression model for spam detection. To do this, a collection of labeled SMS messages collected for mobile phone spam research will be used. This dataset comprises of 5,574 real English non-encoded messages, tagged according to whether they are legitimate (*`ham`*) or spamming (*`spam`*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's look at how to perform shallow learning for spam detection:\n",
    "We'll use the `LogiTextClassifier.py` file that's already been provided to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Logistic regression analysis is a method for estimating the regression function that best links the probability of a dichotomous attribute with a set of explanatory variables. **Logistic assault** is a **nonlinear regression model that's used when the dependent variable is dichotomous**. The objective of the model is to establish the probability with which an observation can generate one or the other values of the dependent variable; it can also be used to classify the observations into two categories, according to their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "In addition to the measurement scale of the dependent variable, logistic regression analysis is distinguished from linear regression because a normal distribution of y is assumed for this, whereas if y is dichotomous, its distribution is obviously binomial. Similarly, in linear regression analysis, the y estimate obtained from the regression varies from $-\\infty$ to $+\\infty$, while in logistic regression analysis, the y estimate varies between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ac4613ae4247844c12aaa6a0684fc01719fcfaa01de7cc764ac1c94816adf3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
