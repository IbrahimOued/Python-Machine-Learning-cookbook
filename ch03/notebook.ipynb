{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictive modeling** is probably one of the most exciting fields in data analytics. It has gained a lot of attention in recent years due to massive amounts of data being available in many different verticals. It is very commonly used in areas concerning data mining to forecast future trends.\n",
    "\n",
    "Predictive modeling is an analysis technique that is used to predict the future behavior of a system. It is a collection of algorithms that can identify the relationship between independent input variables and the target responses. We create a mathematical model, based on observations, and then use this model to estimate what's going to happen in the future.\n",
    "\n",
    "In predictive modeling, we need to collect data with known responses to train our model. Once we create this model, we validate it using some metrics, and then use it to predict future values. We can use many different types of algorithms to create a predictive model. In this chapter, we will use SVMs to build linear and nonlinear models.\n",
    "\n",
    "A predictive model is built using a number of features that are likely to influence the behavior of the system. For example, to estimate weather conditions, we may use various types of data, such as temperature, barometric pressure, precipitation, and other atmospheric processes. Similarly, when we deal with other types of systems, we need to decide what factors are likely to influence its behavior and include them as part of the feature vector before training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a linear classifier using SVMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are **supervised learning models** that we can use to create classifiers and regressors. An SVM solves a system of **mathematical equations and finds the best separating boundary between 2 sets of points**. Let's see how to build a linear classifier using an SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's visualize our data to understand the problem at hand. We will use the `svm.py` file for this. Before we build the SVM, let's understand our data. We will use the data_multivar.txt file that's already provided to you. Let's see how to to visualize the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "In this recipe, we will learn how to build a linear classifier using SVMs\n",
    "see the `svm.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "SVMs are **a set of supervised learning methods** that can be used for b**oth classification and regression**. Given two classes of **linearly separable multidimensional patterns**, **among all the possible separating hyperplanes, the SVM algorithm determines `the one able to separate the classes with the greatest possible margin`**. The margin is the **minimum distance of the points in the two classes** in the training set from the hyperplane identified. \n",
    "\n",
    "Maximization of the margin is linked to generalization. If the training set patterns are classified with a large margin, you can hope that even test-set patterns close to the boundary between the classes are managed correctly. In the following, you can see three lines (**l1**, **l2**, and **l3**). Line **l1** does not separate the two classes, line **l2** separates them, but with a small margin, while line **l3** maximizes the distance between the two classes:\n",
    "\n",
    "![](svm.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs can be used **to separate classes that cannot be separated with a linear classifier**. Object coordinates are mapped into a space called a **feature space** using non-linear functions, called **characteristic functions**. This space is highly multidimensional, in which the two classes can be separated with a linear classifier. So, the initial space is remapped in the new space, at which point the classifier is identified and then returned to the initial space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "**SVMs** constitute a class of learning machines recently introduced in the literature. SVMs derive from concepts concerning the statistical theory of learning and present theoretical generalization properties. The theory that governs the functioning mechanisms of SVMs was introduced by Vapnik in 1965 (statistical learning theory), and was more recently perfected, in 1995, by Vapnik himself, and others. SVMs are one of **the most widely used tools for pattern classification**. Instead of estimating the probability densities of classes, Vapnik suggests directly solving the problem of interest, that is, to determine the decisional surfaces between the classes (classification boundaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a non linear classifier using SVMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A SVM provides a variety of options to build a nonlinear classifier. We need to build a **nonlinear classifier using various kernels**. In this recipe, let's consider two cases here. When we want to represent a curvy boundary between two sets of points, we can either do this using a **polynomial function** or **a radial basis function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the same file used in the previous recipe, Building a linear classifier using SVMs, but in this case, we will use a different kernel to deal with a markedly nonlinear problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a nonlinear classifier using SVMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we have used an SVM classifier to find **the best separating boundary between a dataset of points by solving a system of mathematical equations**. **To address a nonlinear problem, we used Kernel method**s. Kernel methods are thus named for Kernel functions, which are used to operate in the feature space **without calculating data coordinates in space**, but rather **by calculating the internal product between images of all copies of data in the function space**. The calculation of the internal product is often computationally cheaper than the explicit calculation of the coordinates. This method is called the **Kernel stratagem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The main point of the SVM is that **a generic problem can always be solved as long as you carefully choose the kernel and all its parameters**—for example, going to make a total overfitting of the input dataset. The **problem with this method is that it scales quite badly with the size of the dataset**, as it is classically attributed to a D2 factor, even if, in this sense, faster implementations can be obtained by optimizing this aspect. **The problem is identifying the best kernel and providing it with the best parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tackling class imabalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we dealt with problems where we had a similar number of datapoints in all our classes. **In the real world, we might not be able to get data in such an orderly fashion**. Sometimes, **the number of datapoints in one class is a lot more than the number of datapoints in other classes**. If this happens, then **the classifier tends to get biased**. The boundary won't reflect the true nature of your data, just because there is a big difference in the number of datapoints between the two classes. Therefore, it is important **to account for this discrepancy and neutralize it so that our classifier remains impartial**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use a new dataset, named `data_multivar_imbalance.txt`, in which there are three values for each line; the first two represent the coordinates of the point, the third, the class to which the point belongs. Our aim is, once again, to build a classifier, but this time, we will have to face a data-balancing problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "We'll code it in the `svm_imbalanced.py` file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we have used a **SVM classifier to find the best separating boundary between a dataset of points**. To address a **data-balancing problem, we once again used the linear Kernel method**, but we implemented a class_weight keyword in the fit method. The **class_weight variable is a dictionary in the form `{class_label: value}`, where value is a floating-point number greater than 0 that modifies the C parameter of the class (class_label)**, setting it with a new value, obtained by multiplying the old C value with that specified in the value attribute ($C \\times value$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "$C$ is a hyperparameter that determines the penalty for the incorrect classification of an observation. So, we used a weight for the classes to manage unbalanced classes. In this way, we will assign a new value of $C$ to the classes, defined as follows:\n",
    "\n",
    "$C_i = C \\times w_i$\n",
    "\n",
    "Where C is the penalty, wi is a weight inversely proportional to class $i$'s frequency, and $C_i$ is the $C$ value for class $i$. This method suggests increasing the penalty to classify the less represented classes so as to prevent them from being outclassed by the most represented class. \n",
    "\n",
    "In the scikit-learn library, when using SVC, we can set the values for $C_i$ automatically by setting `class_weight='balanced'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extracing confidence measurements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be **nice to know the confidence with which we classify unknown data**. When a new datapoint is classified into a known category, we can **train the SVM to compute the confidence level of that output as well**. A **confidence level** refers to **the probability that the value of a parameter falls within a specified range of values**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use an SVM classifier to find the best separating boundary between a dataset of points. In addition, we will also perform a measure of the confidence level of the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to extract confidence measurements. We will be working in the `svm_confidence.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we built a classifier based on SVM. Once the classifier was obtained, we **used a set of points to measure the distance of those points from the boundary** and then **measured the confidence levels for each of those points**. **When estimating a parameter**, the simple **identification of a single value is often not sufficient**. It is therefore advisable to **accompany the estimate of a parameter with a plausible range of values ​​for that parameter**, which is defined as the confidence interval. It is therefore associated with a cumulative probability value that indirectly, in terms of probability, characterizes its amplitude with respect to the maximum values ​​assumed by the random variable that measures the probability that the random event described by that variable in question falls into this interval and is equal to this area graphically, subtended by the probability distribution curve of the random variable in that specific interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The confidence interval **measures the reliability of a statistic**, such as an opinion poll. For example, **if 40% of the sample interviewed declare to choose a certain product**, it **can be inferred with a level of confidence of 99% that a percentage between 30 and 50 of the total consumer population will be expressed in favor of that product**. From the same sample interviewed, **with a 90% confidence interval**, it **can be assumed that the percentage of opinions favorable to that product is now between 37% and 43%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Finding  optimal hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous chapter, hyperparameters are important for determining the performance of a classifier. Let's see how to extract optimal hyperparameters for SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In machine learning algorithms, **various parameters are obtained during the learning process**. In contrast, **hyperparameters are set before the learning process begins**. Given these hyperparameters, the training algorithm learns the parameters from the data. In this recipe, we will extract hyperparameters for a model based on an SVM algorithm using the grid search method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to find optimal hyperparameters. We'll use the `perform_grid_search.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In the previous recipe, *Building a nonlinear classifier using SVMs*, we repeatedly modified the kernel of the SVM algorithm to obtain an improvement in the classification of data. On the basis of the hyperparameter definition given at the beginning of the recipe, **it is clear that the kernel represents a hyperparameter**. In this recipe, we **randomly set the value for this hyperparameter and checked the results to find out which value determines the best performance**. However, **a random selection of algorithm parameters may be inadequate**.\n",
    "\n",
    "Furthermore, it is **difficult to compare the performance of different algorithms by setting the parameters randomly**, because **an algorithm can perform better than another with a different set of parameters**. And **if the parameters are changed, the algorithm may have worse results than the other algorithms**.\n",
    "\n",
    "As a result, **the random selection of parameter values ​​is not the best approach we can take to find the best performance for our model**. On the contrary, it would be **advisable to develop an algorithm that automatically finds the best parameters for a particular model**. There are several methods for searching for hyperparameters, such as the following: `grid search, randomized search, and Bayesian optimization`.\n",
    "\n",
    "NB: I had a book on how to tune hyperparameters, but dunno where it is, gotta find it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **The grid search algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **grid search** algorithm does this by automatically looking for the set of hyperparameters that detracts from the best performance of the model.\n",
    "\n",
    "The `sklearn.model_selection.GridSearchCV()` function performs an **exhaustive search over specified parameter values for an estimator**. **Exhaustive search** (also named direct search, or brute force) is a comprehensive examination of all possibilities, and therefore represents an efficient solution method in which every possibility is tested to determine whether it is the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The randomized search algorithm\n",
    "Unlike the GridSearchCV method, **not all parameter values are tested in this method**, but the **parameter settings are sampled in a fixed number**. The parameter settings that are tested are set through the n_iter attribute. Sampling without replacement is performed if the parameters are presented as a list. If at least one parameter is supplied as a distribution, substitution sampling is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The aim of a Bayesian hyperparameter optimizer is to **construct a probability model of the objective function and use it to select the hyperparameters that work best for use in the real objective function**. Bayesian statistics allow us to **foresee not only a value, but a distribution, and this is the success of this methodology**.\n",
    "\n",
    "The Bayesian method, when compared with the two methods already dealt with (grid search and random search), **`stores the results of the past evaluation, which it uses to form a probabilistic model that associates the hyperparameters with a probability of a score on the objective function`**.\n",
    "\n",
    "This model is called a **surrogate** of the objective function and is much easier to optimize than the objective function itself. This result is obtained by following this procedure:\n",
    "\n",
    "1. A surrogate probability model of the objective function is constructed.\n",
    "2. The hyperparameters that give the best results on the surrogate are searched.\n",
    "3. These hyperparameters are applied to the real objective function.\n",
    "4. The surrogate model is updated by incorporating the new results.\n",
    "5. Repeat steps 2–4 until you reach the pre-established iterations or the maximum time.\n",
    "\n",
    "In this way, the surrogate probability model is updated after each evaluation of the objective function. To use a Bayesian hyperparameter optimizer, several libraries are available: *scikit-optimize*, *spearmint*, and *SMAC3*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Commonly, hyperparameters are all those values that can be freely set by the user, and that are generally optimized, maximizing the accuracy on the validation data with appropriate research. Even the choice of a technique rather than another can be seen as a categorical hyperparameter, which has as many values as the methods we can choose from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a event predictor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply all of this knowledge from this chapter to a real-world problem. We will build an SVM to predict the number of people going in and out of a building. The dataset is available at [this url](https://archive.ics.uci.edu/ml/datasets/CalIt2+Building+People+Counts). We will use a slightly modified version of this dataset so that it's easier to analyze. The modified data is available in the *building_event_binary.txt* and the *building_event_multiclass.txt* files that are already provided to you. In this recipe, we will learn how to build an event predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's understand the data format before we start building the model. Each line in building_event_binary.txt consists of six comma-separated strings. The ordering of these six strings is as follows:\n",
    "\n",
    "* Day\n",
    "* Date\n",
    "* Time\n",
    "* The number of people going out of the building\n",
    "* The number of people coming into the building\n",
    "* **The output indicating whether or not it's an event**\n",
    "\n",
    "The first five strings form the input data, and our task is to predict whether or not an event is going on in the building.\n",
    "\n",
    "Each line in *building_event_multiclass.txt* consists of six comma-separated strings. This is more granular than the previous file, in the sense that the output is the exact type of event going on in the building. The ordering of these six strings is as follows:\n",
    "\n",
    "* Day\n",
    "* Date\n",
    "* Time\n",
    "* The number of people going out of the building\n",
    "* The number of people coming into the building\n",
    "* **The output indicating the type of event**\n",
    "\n",
    "The first five strings form the input data, and our task is to predict what type of event is going on in the building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build and event predictor. We'll code it in the `event.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we used data obtained from observations of people who flowed in and out of a building during 15 weeks, and at 48 time intervals per day. We therefore built a classifier able to predict the presence of an event such as a conference in the building, which determines an increase in the number of people present in the building for that period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Later in the recipe, we used the same classifier on a different database to also predict the type of event that is held within the building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Estimating traffic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting application of SVMs is to predict traffic, based on related data. In the previous recipe, we used an SVM as a classifier. In this recipe, we will use an SVM as a regressor to estimate the traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gettting ready\n",
    "We will use the dataset available at [this link](https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor). This is a dataset that counts the number of cars passing by during baseball games at the Los Angeles Dodgers home stadium. We will use a slightly modified form of that dataset so that it's easier to analyze. You can use the traffic_data.txt file, already provided to you. Each line in this file contains comma-separated strings formatted in the following manner:\n",
    "\n",
    "* Day\n",
    "* Time\n",
    "* The opponent team\n",
    "* Whether or not a baseball game is going on\n",
    "* The number of cars passing by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to estimate traffic, we'll be working in the `traffic.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we used data collected by a sensor on the 101 North Highway in Los Angeles, near the stadium where the Dodgers play. This position is sufficiently close to the stadium to detect the increase in traffic that occurs during a match.\n",
    "\n",
    "The observations were made over 25 weeks, over 288 time intervals per day (every 5 minutes). We built a regressor based on the SVM algorithm to predict the presence of a baseball game at the Dodgers stadium. In particular, we can estimate the number of cars that pass that position on the basis of the value assumed by the following predictors: day, time, the opponent team, and whether or not a baseball game is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "**Support vector regression (SVR)** is based on the same principles as SVMs. In fact, SVR is adapted from SVMs, where the dependent variable is numeric rather than categorical. One of the main advantages of using SVR is that it is a nonparametric technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simplifying ML workflow using TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow** is an open source numerical calculation library. The library was created by Google programmers. It provides all the tools necessary to build deep learning models and offers developers a black-box interface to program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will introduce the TensorFlow framework, using a simple neural network to classify the iris species. We will use the iris dataset, which has 50 samples from the following species:\n",
    "\n",
    "* Iris setosa\n",
    "* Iris virginica\n",
    "* Iris versicolor\n",
    "\n",
    "Four features are measured from each sample, namely the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "The following variables are contained:\n",
    "\n",
    "* Sepal length in cm\n",
    "* Sepal width in cm\n",
    "* Petal length in cm\n",
    "* Petal width in cm\n",
    "* Class: setosa, versicolor, or virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to simplify machine learning workflow using TensorFlow:\n",
    "We'll code in the `IrisTensorflow.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we used the *tensorflow* library to build a simple neural network to classify iris species from four features measured. In this way, we saw how simple it is to implement a model based on a machine learning algorithm using the tensorflow library. This topic, and on deep neural networks in general, will be analyzed in detail in **Chapter 13**, Deep Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "TensorFlow provides native APIs in Python, C, C++, Java, Go, and Rust. The third-party APIs available are in C#, R, and Scala. Since October 2017, it has integrated eager execution functionality which allows the immediate execution of the operations referred to by Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing a stacking method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A combination of different approaches leads to better results**: this statement works in different aspects of our life and also adapts to algorithms based on machine learning. **Stacking is the process of combining various machine learning algorithms**. This technique is due to David H. Wolpert, an American mathematician, physicist, and computer scientist. \n",
    "\n",
    "In this recipe, we will learn how to implement a stacking method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will use the `heamy` library to stack the two models that we just used in the previous recipes. **`The heamy library is a set of useful tools for competitive data science`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to implement a stacking method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Stacked generalization works by **deducing the biases of the classifier/regressor relative to a supplied learning dataset**. This **deduction works by generalizing into a second space whose inputs are the hypotheses of the original generalizers and whose output is the correct hypothesis**. When **used with multiple generators**, stacked generalization is an **alternative to cross-validation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Stacking tries to **exploit the advantages of each algorithm by ignoring or correcting their disadvantages**. It can be seen as **a mechanism that corrects errors in your algorithms**. *Another library to perform a stacking procedure is* `StackNet`. \n",
    "\n",
    "StackNet is a **framework implemented in Java based on Wolpert's stacked generalization on multiple levels to improve accuracy in machine learning predictive problems**. The StackNet model functions as a neural network in which **the transfer function takes the form of any supervised machine learning algorithm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5052bdfd4928f465c492d21ff3d7daf709b24acb81685a4e868a1ee9dbe33934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
