{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning** is a paradigm in machine learning where we build models without relying on labeled training data. Up to this point, we have dealt with data that was labeled in some way. This means that learning algorithms can look at this data and learn to categorize it them based on labels. In the world of unsupervised learning, we don't have this opportunity! These algorithms are used when we want to find subgroups within datasets using a similarity metric.\n",
    "\n",
    "In unsupervised learning, information from the database is automatically extracted. All this takes place without prior knowledge of the content to be analyzed. In unsupervised learning, there is no information on the classes that the examples belong to, or on the output corresponding to a given input. We want a model that can discover interesting properties, such as groups with similar characteristics, which happens in **clustering**. An example of the application of these algorithms is a search engine. These applications are able to create a list of links related to our search, starting from one or more keywords. \n",
    "\n",
    "These algorithms work by comparing data and looking for similarities or differences. The validity of these algorithms depends on the usefulness of the information they can extract from the database. Available data only concerns the set of features that describe each example.\n",
    "\n",
    "One of the most common methods is clustering. You will have heard this term being used quite frequently; we mainly use it for data analysis when we want to find clusters in our data. These clusters are usually found by using a certain kind of similarity measure, such as the Euclidean distance. Unsupervised learning is used extensively in many fields, such as data mining, medical imaging, stock market analysis, computer vision, and market segmentation.****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clustering data using the $k$-means algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **$k$-means algorithm is one of the most popular clustering algorithms**. This algorithm is used **to divide the input data into k subgroups using various attributes of the data**. Grouping is achieved using an **optimization technique where we try to minimize the sum of squares of distances between the datapoints and the corresponding centroid of the cluster**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will use the $k$-means algorithm to group the data into **four clusters identified by the relative centroid**. We will also be able to trace the boundaries to identify the areas of relevance of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to perform a clustering data analysis using the k-means algorithm. We'll code in the `kmeans.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "$K$-means was developed by James MacQueen, who, in 1967, designed it for the purpose of dividing groups of objects into k partitions based on their attributes. It is a variation of the **expectation-maximization (EM) algorithm**, whose **objective is to determine the groups of k data generated by Gaussian distributions**. The difference between the two algorithms lies in **the Euclidean distance calculation method**. **In k-means, it is assumed that the attributes of the object can be represented as vectors**, and thus **form a vector space**. The goal is to **minimize the total intra-cluster variance (or standard deviation)**. Each cluster is identified by a centroid.\n",
    "\n",
    "The algorithm follows an iterative procedure, as follows:\n",
    "\n",
    "1. Choose the number of k clusters\n",
    "2. Initially, create k partitions and assign each entry partition either randomly, or by using some heuristic information\n",
    "3. Calculate the centroid of each group\n",
    "4. Calculate the distance between each observation and each cluster centroid\n",
    "5. Then, construct a new partition by associating each entry point with the cluster whose centroid is closer to it\n",
    "6. The centroid for new clusters is recalculated\n",
    "7. Repeat steps 4 to 6 until the algorithm converges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The purpose of the algorithm is **to locate $k$ centroids**, **one for each cluster**. **The position of each centroid is of particular importance as different positions cause different results**. The best choice is to **put them as far apart as possible from each other**. When this is done, you must associate each object with the nearest centroid. In this way, we will get a first grouping. After finishing the first cycle, we go to the next one by recalculating the new k centroids as the cluster's barycenter using the previous one. Once you locate these new $k$ centroids, you need to make a new connection between the same dataset and the new closest centroid. At the end of these operations, a new cycle is performed. Due to this cycle, we can note that **the $k$ centroids change their position step by step until they are modified**. So, **the centroid no longer moves**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compressing an image using vector quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main applications of k-means clustering is **vector quantization**. Simply speaking, **vector quantization is the N-dimensional version of rounding off**. When we deal with one-dimensional data, such as numbers, we use the rounding-off technique to reduce the memory needed to store that value. For example, **instead of storing 23.73473572**, **we just store 23.73** if we want to be accurate up to the second decimal place. **Or, we can just store 24 if we don't care about decimal places**. It **depends on our needs and the trade-off that we are willing to make**.\n",
    "\n",
    "Similarly, **when we extend this concept to N-dimensional data**, **it becomes vector quantization**. Of course, there are more nuances to it! **Vector quantization is popularly used in image compression where we store each pixel using fewer bits than the original image to achieve compression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "we will use a sample image and then we will compress the image further by reducing the number of bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to compress an image using vector quantization\n",
    "We'll be coding in the `vector_quantization.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Vector quantization is **an algorithm used for signal compression**, **image coding**, and **speech**. We use **geometric criteria (the Euclidean distance) to find clusters**. It is, therefore, **an example of unsupervised training**. It is a technique that allows **the modeling of probability density functions through the distribution of prototype vectors**. Vector quantization **divides a large set of points (vectors) into clusters by using a similar number of points closer to them**. Each cluster is illustrated by its centroid point (as in k-means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The vector quantization algorithm can be used to divide a dataset into a number of clusters. The algorithm is based on the calculation of the Euclidean distance for the allocation of the samples to the cluster, to which it belongs. The algorithm consists of the following steps:\n",
    "\n",
    "1. At the beginning, all the vectors are assigned to the same cluster, whose centroid is calculated as the mean value of all the vectors.\n",
    "2. For each centroid, a perturbation is introduced that generates two new cluster centers. The old representative is discarded.\n",
    "3. Each carrier is reassigned to one of the new clusters according to the minimum distance criterion.\n",
    "4. The new representatives are calculated as the average value of the vectors assigned to each cluster. These will be the new centers of the cluster.\n",
    "5. If the end criterion is met, the algorithm terminates. If not, return to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping data using agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we talk about agglomerative clustering, we need to understand **hierarchical clustering**. Hierarchical clustering refers to a **set of clustering algorithms that creates tree-like clusters by consecutively splitting or merging them**, and they are represented using a tree. Hierarchical clustering algorithms can be **either bottom-up or top-down**. Now, what does this mean? In **bottom-up algorithms**, **each datapoint is treated as a separate cluster with a single object**. These clusters are then **successively merged until all the clusters are merged into a single giant cluster**. This is called **agglomerative clustering**. On the other hand, **top-down algorithms start with a giant cluster and successively split these clusters until individual datapoints are reached**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In hierarchical clustering, we construct clusters by partitioning the instances recursively using a top-down or bottom-up fashion. We can divide these methods as follows:\n",
    "\n",
    "* **Agglomerative algorithm (bottom-up)**: Here, we obtain the solution from individual statistical units. At each iteration, we aggregate the most closely-related statistical units and the procedure ends when a single cluster is formed.\n",
    "* **Divisive algorithm (top-down)**: Here, all units are in the same class and the unit that is not similar to others is added to a new cluster for each subsequent iteration.\n",
    "\n",
    "Both methods result in a `dendrogram`. This represents a nested group of objects, and the similarity levels at which the groups change. By cutting the dendrogram at the desired similarity level, we can get a clustering of data objects. The merging or division of clusters is performed using a similarity measure, which optimizes a criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to group data using agglomerative clustering. We'll be working in the `agglomerative.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In agglomerative clustering, each observation begins in its cluster and the clusters are subsequently combined. The strategies for joining the clusters are as follows:\n",
    "\n",
    "* **Ward clustering minimizes the sum of squared differences within all the clusters**.\n",
    "* **Maximum or complete linkage** is used to **minimize the maximum distance between observations of pairs of clusters**.\n",
    "* **Average linkage** is used to **minimize the average of the distances between all observations of pairs of clusters**.\n",
    "* **Single linkage** is used to **minimize the distance between the closest observations of pairs of clusters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To **decide what clusters must be combined**, it is necessary to **define a measure of dissimilarity between the clusters**. In most hierarchical clustering methods, specific metrics are used to quantify the distance between two pairs of elements, and a linking criterion that defines the dissimilarity of two sets of elements (clusters) as a function of the distance between pairs of elements in the two sets.\n",
    "\n",
    "These common metrics are as follows:\n",
    "\n",
    "* The Euclidean distance\n",
    "* The Manhattan distance \n",
    "* The uniform rule\n",
    "* The Mahalanobis distance, which corrects data by different scales and correlations in variables\n",
    "* The angle between the two vectors\n",
    "* The Hamming distance, which measures the minimum number of substitutions required to change one member into another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the performance of clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have built different clustering algorithms, but haven't measured their performance. In supervised learning, the predicted values with the original labels are compared to calculate their accuracy. In contrast, in unsupervised learning, we have no labels, so we need to find a way to measure the performance of our algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "A good way to measure a clustering algorithm is by seeing how well the clusters are separated. Are the clusters well separated? Are the datapoints in a cluster that is tight enough? We need a metric that can quantify this behavior. We will use a metric called the silhouette coefficient score. This score is defined for each datapoint; this coefficient is defined as follows:\n",
    "\n",
    "$score = \\frac{x - y}{max(x,y)}$\n",
    "\n",
    "Here, **$x$** is the average distance between the current datapoint and all the other datapoints in the same cluster, and **$y$** is the average distance between the current datapoint and all the datapoints in the next nearest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to evaluate the performance of clustering algorithms. We'll be working in the `performance.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The `sklearn.metrics.silhouette_score` function computes the mean silhouette coefficient of all the samples. For each sample, two distances are calculated: the mean intra-cluster distance $(x)$, and the mean nearest-cluster distance $(y)$. The silhouette coefficient for a sample is given by the following equation:\n",
    "\n",
    "$score = \\frac{x - y}{max(x,y)}$\n",
    "\n",
    "Essentially, $y$ is the distance between a sample and the nearest cluster that does not include the sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The best value is $1$, and the worst value is $-1$. **$0$ represents clusters that overlap**, while **values of less than $0$ mean that that particular sample has been attached to the wrong cluster**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the number of clusters using the DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we discussed the k-means algorithm, we saw that we had to give the number of clusters as one of the input parameters. In the real world, **we won't have this information available**. We can definitely **sweep the parameter space to find out the optimal number of clusters using the silhouette coefficient score**, but this will be an expensive process! **A method that returns the number of clusters in our data will be an excellent solution to the problem**. DBSCAN does just that for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will perform a **DBSCAN analysis** using the `sklearn.cluster.DBSCAN` function. We will use the same data that we used in the previous Evaluating the performance of clustering algorithms (data_perf.txt) recipe, to compare the two methods used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to automatically estimate the number of clusters using the DBSCAN algorithm. We'll be working in the `estimate_clusters.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "DBSCAN works by **treating datapoints as groups of dense clusters**. If a point belongs to a cluster, then there **should be a lot of other points that belong to the same cluster**. One of the parameters that we can control is **the maximum distance of this point from other points**. This is called `epsilon`. **No two points in a given cluster should be further away than epsilon**. One of the main advantages of this method is that it **can deal with outliers**. If there are some points located alone in a low-density area, DBSCAN will detect these points as outliers as opposed to forcing them into a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "DBSCAN presents the following pros and cons:\n",
    "\n",
    "| Pro  | Cons          | \n",
    "|:-------------- |:--------------- |\n",
    "| It does not require to know the number of a priori clusters.  |   The quality of clustering depends on its distance measurement.       |\n",
    "| It can find clusters of arbitrary forms.  | It is not able to classify datasets with large differences in density.             |\n",
    "| It requires only two parameters.  |           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding patterns in stock market data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use unsupervised learning for stock market analysis. Since we don't know how many clusters there are, we'll use an algorithm called **affinity propagation (AP)** on the cluster. It tries to find a representative datapoint for each cluster in our data, along with measures of similarity between pairs of datapoints, and considers all our datapoints as potential representatives, also called **exemplars**, of their respective clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will analyze the stock market variations of companies over a specified duration. Our goal is to then find out **what companies behave similarly in terms of their quotes over time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to find patterns in stock market data. We'll be working in the `stock_market.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "AP is a clustering algorithm **based on the concept of passing messages between points (item)**. Unlike clustering algorithms such as k-means, **AP does not require the cluster number to be defined a priori**. **AP searches for representative members (exemplars) of the set of inputs, which are, in fact, representative of the individual clusters**.\n",
    "\n",
    "**The central point of the AP algorithm is the identification of a subset of exemplars**. In the input, a matrix of similarity is taken between pairs of data. The data exchanges real values as messages until suitable specimens emerge, and consequently, good clusters are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To perform AP clustering, the `sklearn.cluster.affinity_propagation()` function was used. In the **case of training samples with similar similarities and preferences**, the assignment of cluster centers and labels **depends on preference**. If the **preference is less than the similarities**, **a single cluster center and a 0 label for each sample will be returned**. Otherwise, **each training sample becomes its cluster center and a unique mark is assigned**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a customer segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main applications of unsupervised learning is **market segmentation**. This is when we **don't have labeled data available all the time**, but it's important to segment the market so that people can **target individual groups**. This is very useful in **advertising**, **inventory management**, **implementing strategies for distribution**, and **mass media**. Let's go ahead and apply unsupervised learning to one such use case to see how it can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will be dealing with a wholesale vendor and his customers. We will be using the data available at [this link](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). The spreadsheet contains data regarding the consumption of different types of items by their customers and our goal is to find clusters so that they can optimize their sales and distribution strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a customer segmentation model. We'll be working in the `customer_segmentation.py` file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we have faced a clustering problem `by using the mean shift algorithm`. It is a **clustering type that assigns datapoints to clusters in an iterative manner by moving points to the mode**. **The mode is the value that appears most frequently**.\n",
    "\n",
    "The algorithm assigns iteratively each data point to the centroid of the nearest cluster. The centroid of the nearest cluster is determined by where most of the neighboring points are located. Thus, at each iteration, each data point approaches the point where the greatest number of points is located, which is, or will lead to, the cluster center. When the algorithm stops, each point is assigned to a cluster. Unlike the k-means algorithm, the mean shift algorithm is not required in advance to specify the number of clusters; this is determined automatically by the algorithm. The mean shift algorithm is widely used in the field of image processing and artificial vision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To perform the **mean shift clustering**, a `sklearn.cluster.MeanShift()` function was used. This function **carries out a mean shift clustering using a flat kernel**. The mean shift clustering allows us to **identify point aggregates in a uniform density of samples**. **Candidates for the centroids are updated with the average of points within a given region**. These points are then filtered in a postprocessing phase to eliminate possible duplicates to form the final set of centroids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using autoencoders to reconstruct handwritten digit images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An autoencoder is a neural network whose purpose is to code its input into small dimensions**, and for the result that is obtained **to be able to reconstruct the input itself**. Autoencoders are made up by **the union of the following two subnets: encoder and decoder**. A **loss function is added to these functions and it is calculated as the distance between the amount of information loss between the compressed representation of the data and the decompressed representation**. The encoder and the decoder will be **differentiable with respect to the distance function**, so the parameters of the encoding and decoding functions **can be optimized to minimize the loss of reconstruction, using the gradient stochastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "**Handwriting recognition (HWR)** is widely used in modern technology. The written text image can be taken offline from a piece of paper by optical scanning (**optical character recognition, OCR**), or intelligent word recognition. Calligraphy recognition shows the ability of a computer to receive and interpret input that can be understood by hand from sources such as paper documents, touchscreens, photographs, and other devices. HWR consists of various techniques that generally require OCR. However, a complete script recognition system also manages formatting, carries out correct character segmentation, and finds the most plausible words.\n",
    "\n",
    "The **Modified National Institute of Standards and Technology (MNIST)** is a large database of handwritten digits. It has a set of 70,000 examples of data. It is a subset of MNIST's larger dataset. The digits are of 28 x 28 pixel resolution and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each pixel value from the 28 x 28 matrix, and one value is the actual digit. The digits have been size-normalized and centered in a fixed-size image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build autoencoders to reconstruct handwritten digit images\n",
    "We'll be workinh in the `autoencmninst.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "An autoencoder is a neural network whose purpose is to code its input into small dimensions and the result obtained so as to be able to reconstruct the input itself. Autoencoders are made up of a union of the following two subnets.\n",
    "\n",
    "First, we have an encoder that calculates the following function:\n",
    "\n",
    "$z = \\phi(x)$\n",
    "\n",
    "Given an $x$ input, the encoder encodes it in a $z$ variable, which is also called a latent variable. $z$ usually has much smaller dimensions than $x$.\n",
    "\n",
    "Second, we have a decoder that calculates the following function:\n",
    "\n",
    "$x' = \\psi(z)$\n",
    "\n",
    "Since $z$ is the code of $x$ produced by the encoder, the decoder must decode it so that $x'$ is similar to $x$. The training of autoencoders is intended to minimize the mean squared error between the input and the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Keras is a Python library that provides a simple and clean way to create a range of deep learning models. The Keras code was released under the MIT license. Keras has been structured based on austerity and simplicity, and it provides a programming model without ornaments to maximize readability. It allows neural networks to be expressed in a very modular way, considering models as a sequence or a single graph."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5426e2286ce2e419dfccc637b93561cd5b8316b00ac074413e19632050d39113"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uvbf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
