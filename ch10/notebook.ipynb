{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Image Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computer vision** is a field that studies how to process, analyze, and understand the contents of visual data. In image content analysis, we use a lot of computer vision algorithms to build our understanding of the objects in the image. Computer vision covers various aspects of image analysis, such as **object recognition**, **shape analysis**, **pose estimation**, **3D modeling**, **visual search**, and so on. Humans are really good at identifying and recognizing things around them! The ultimate goal of computer vision is to accurately model the human vision system using computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer vision consists of various levels of analysis. In low-level vision, we deal with pixel-processing tasks, such as **edge detection**, **morphological processing**, and **optical flow**. In middle-level and high-level vision, we deal with things such as **object recognition, 3D modeling, motion analysis**, and various other aspects of visual data. As we go higher, we tend to delve deeper into the conceptual aspects of our visual system and try to extract a description of visual data, based on activities and intentions. One thing to note is that higher levels tend to rely on the outputs of the lower levels for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common questions here is this: how is computer vision different than image processing? **Image processing** studies image transformations at the pixel level. Both the input and output of an image processing system are images. Some common examples are edge detection, **histogram equalization**, and **image compression**. **Computer vision algorithms heavily rely on image processing algorithms to perform their duties**. In computer vision, we deal with **more complex things that include understanding the visual data at a conceptual level**. The reason for this is that we want to **construct meaningful descriptions of the objects in the images**. The **output of a computer vision system is an interpretation of the 3D scene in the given image**. This interpretation can come in various forms, depending on the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operating on images using OpenCV-Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will use a library called Open Source Computer Vision Library (OpenCV), to analyze images. OpenCV is the world's most popular library for computer vision. As it has been highly optimized for many different platforms, it has become the de facto standard in the industry. Before you proceed, make sure that you install the library with Python support. You can download and install OpenCV at http://opencv.org. For detailed installation instructions on various operating systems, you can refer to the documentation section on the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will take a look at how to operate on images using OpenCV-Python. In this recipe, we will look at how to load and display an image. We will also look at how to crop, resize, and save an image to an output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how we can operate on images using OpenCV-Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the file `operating_on_images.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we learned how to operate on images using the OpenCV-Python library. The following tasks were performed\n",
    "* Loading and displaying an image\n",
    "* Cropping an image\n",
    "* Resizing an image\n",
    "* Saving an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "OpenCV is a free software library that was originally developed by Intel and the Nizhny Novgorod research center in Russia. Later, it was maintained by Willow Garage and is now maintained by Itseez. The programming language that's mainly used to develop with this library is C ++, but it is also possible to interface through C, Python, and Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge detection** is one of the most popular techniques in computer vision. It is used as a preprocessing step in many applications. With edge detection, you can mark points in a digital image **where light intensity suddenly changes**. The sudden changes in the properties of an image want to highlight important events or changes in the physical word of wich the images are representations. Theses changes identify, for example, **surface orientation discontinuities**, **depth discontinuities**, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we'll learn how to use different edge detectors to detect edges in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how.\n",
    "We'll use the `edge_detector.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 The **SOBEL FILTER** is a type of edge detector that uses a $3 \\times 3$ kernel to detect horizontal and vertical edges separatedely.\n",
    "\n",
    "6 The **LAPLACIAN EDGE DETECTOR** detects edges in both directions.\n",
    "\n",
    "7 Even though **Laplacian addresses the shortcomings of Sobel**, **the output is still very noisy**. **The Canny edge detector outperforms all of them because of the way it treats the problem**. It is a **multistage process**, and it uses **hysteresis to come up with clean edges**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The **Sobel operator** is a **differential operator**, which calculates an **approximate value of the gradient of a function that represents the brightness of the image**. At each point in the image, the Sobel operator can **correspond to the gradient vector or to the norm of that vector**. The algorithm that's used by the Sobel operator is **based on the convolution of the image with a filter**, separated and of integer value, applied both in the vertical and horizontal direction, and is therefore economical in terms of the calculation power required.\n",
    "\n",
    "**The Laplacian edge detector is part of the zero-crossing methods that look for points where the second-order derivative goes through zero**, which is usually the Laplacian function or a differential expression of a non-linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "**The Canny algorithm uses a multi-stage calculation method to find outlines of many of the types that are normally present in real images**. To do this, the algorithm must identify and mark as many contours as possible in the image good location. Furthermore, the marked contours must be as close as possible to the real contours of the image. Finally, a given image contour must be marked only once, and if possible, the noise that's present in the image must not cause the detection of false contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram equalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogram equalization** is the process of ***modifying the intensities of the image pixels to enhance the image's contrast***. The human eye likes contrast! This is the reason why almost **all camera systems use histogram equalization** to make images look nice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "The interesting thing is that the histogram equalization process is different for grayscale and color images. There's a catch when dealing with color images, and we'll see it in this recipe. Let's see how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do to it\n",
    "Let's see how we can perform histogram equalization,\n",
    "we'll use the `histogram_equalizer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Histogram equalization is a digital image processing method with which you can **calibrate the contrast using the image histogram**. **Histogram equalization increases the general contrast of many images**, particularly **when the usable image data is represented by very close intensity values**. With this adaptation, *intensities can be better distributed on the histogram*. In this way, **the areas with low local contrast obtain a greater contrast**. The equalization of the histogram is achieved by spreading most of the values of frequent intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To equalize the histogram of the color images, we need to follow a different procedure. **Histogram equalization only applies to the intensity channel**. `An RGB image consists of three color channels`, and we **cannot apply the histogram equalization process on these channels separately**. We need to `separate the intensity information from the color information before we do anything`. So, **we convert it into a YUV colorspace first**, **equalize the Y channel**, and then **convert it back into RGB to get the output**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corner detection** is an important process in computer vision. It helps us identify the salient points in the image. This was one of the earliest feature extraction techniques that was used to develop image analysis systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will learn how to detect the corner of a box by placing markers at the points that are identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the `corner_detector.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Corner detection is an approach that's used in computer vision to extract **types of features and infer the contents of the image**. It is often used in **motion detection**, **image recording**, **video tracking**, **image mosaicization**, **image panoramas creation**, **3D modeling**, and **object recognition**. It is a *topic similar to the detection of points of interest*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Corner detection methods can be subdivised into 2 groups:\n",
    "* Techniques **based on the extraction of the contours and the subsequent identification of the points** corresponding to the maximum curvature, or where the edge segments intersect\n",
    "* Algorithms that **search for corners directly from the intensity of the gray levels** of the image pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting SIFT feature points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale invariant feature transform (SIFT)** is one of the most popular features in the field of computer vision. David Lowe first proposed this in his seminal paper. It has since become **one of the most effective features to use for image recognition and content analysis**. It is **robust against scale, orientation, intensity, and so on**. This forms the basis of our object recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will learn how to detect SIFT feature points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how we can detect SIFT feature points\n",
    "We'll use the `feature_detector.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "For each object in an image, some interesting points are extracted to provide a description of the characteristics of the object. This feature, obtained from an image selected for training, is used to identify the object when trying to locate it in a test image that contains many other objects. To obtain a reliable recognition, the features that are extracted from the training image must be detectable, even with scale variations, noise, and lighting. These points are usually placed in high-contrast regions of the image, such as object contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "In `Lowe's method`, the key points of the SIFT objects are extracted from a set of reference images in the first phase and then they are stored in a database. The recognition of the object in a new image takes place by individually comparing each characteristic of the new image with the database that was obtained previously and looking for features based on the Euclidean distance of their feature vectors. From the complete set of matches in the new image, subsets of key points are identified that agree with the object and its position, scale, and orientation to filter the best matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Start feature detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIFT feature detector is good in many cases. However, when we build object recognition systems, we may want to use a different feature detector before we extract features using SIFT. This will give us the flexibility to cascade different blocks to get the best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will use the **Star feature detector** to detect features from an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `start_detector.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we learned how to use the OpenCV-Python library to build a Star feature detector. The following tasks were performed:\n",
    "\n",
    "* Loading an image\n",
    "* Converting to grayscale\n",
    "* Detecting features using the Star feature detector\n",
    "* Drawing keypoints and displaying the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The **Star function detector** is based on **CenSurE (Center Surrounded Extrema).** The differences between the two detectors lie in the choice of polygons:\n",
    "\n",
    "* CenSurE uses square, hexagons, and octagons as an alternative to the circle\n",
    "* Star approximates the circle with two superimposed squares: one vertical and one rotated 45 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating features using Visual Codebook and Vector quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an **object recognition system**, we need to **extract feature vectors from each image**. Each image needs to **have a signature that can be used for matching**. We use a concept called **Visual Codebook to build image signatures**. This codebook is basically **the dictionary that we will use to come up with a representation for the images in our training data image signatures set**. We **use vector quantization to cluster many feature points and come up with centroids**. These **centroids will serve as the elements of our Visual Codebook**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will create features using Visual Codebook and vector quantization. To build a robust object recognition system, you need tens of thousands of images. There is a dataset called `Caltech256` that's very popular in this field! It contains 256 classes of images, where each class contains thousands of samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how we can create features using Visual Codebook and vector quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we used a **visual textbook as a dictionary**, which we then used to **create a representation for images in our image signatures**, which are contained in the training set. So, we used **vector quantization to group many characteristic points and create centroids**. **These centroids are served as elements of our visual textbook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "We extract the features from various points in the image, counting the frequency of the values of the extracted features and classifying the image based on the frequency found, **which is a technique that's similar to the representation of a document in a vector space**. It is a vector quantization process with which I create a dictionary to discretize the possible values of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an image classifier using Extremely Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object recognition system uses an image classifier to classify the images into known categories. **Extremely Random Forests (ERFs)** are very popular in the field of machine learning because of **their speed and accuracy**. This algorithm is based on **decision trees**. Their differences compared to classical decision trees are **in the choice of the points of division of the tree**. The best division to **separate the samples of a node into two groups** is done by **creating random subdivisions for each of the randomly selected features and choosing the best division between those**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will **use ERFs to train our image classifier**. We basically construct decision trees based on our image signatures, and then train the forest to make the right decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how we can train an image classifier using ERFs\n",
    "we'll use the `trainer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, **we used ERFs to train our image classifier**. First, we **defined an argument parser function** and **a class to handle ERF training**. We used a **label encoder to encode our training labels**. Then, we **loaded the feature map we obtained in the Creating features** using Visual Codebook and vector quantization recipe. So, we extracted feature vectors and the labels, and finally we trained the ERF classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To train the image classifier, the `sklearn.ensemble.ExtraTreesClassifier` function was used. This function builds an extremely randomized tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5aa916838d07d7e4f9223a7eec3e82d9bad7fed71266f13131d6a48bba809aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
