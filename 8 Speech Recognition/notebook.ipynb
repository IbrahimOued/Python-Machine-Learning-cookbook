{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speech recognition** refers to the process of recognizing and understanding spoken language. The input comes in the form of audio data, and the speech recognizers will process this data to extract meaningful information from it. This has a lot of practical uses, such as **voice-controlled devices**, the **transcription of spoken language into words** and **security systems**.\n",
    "\n",
    "Speech signals are **very versatile in nature**. There are **many variations of speech in the same language**. There are different elements to speech, such as **language**, **emotion**, **tone**, **noise**, and **accent**. It's **difficult to rigidly define a set of rules of what can constitute speech**. Even with all these variations, humans are very good at understanding all of this with relative ease. Hence, we need machines to understand speech in the same way.\n",
    "\n",
    "Over the last couple of decades, researchers have worked on various aspects of speech, such as identifying the speaker, understanding words, recognizing accents, and translating speech. Among all these tasks, **automatic speech recognition** has been the focal point for many researchers. In this chapter, we will learn how to build a **speech recognizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading an plotting audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how to read an audio file and visualize the signal. This will be a good starting point, and it will give us a good understanding of the basic structure of audio signals. Before we start, we need to understand that audio files are digitized versions of actual audio signals. Actual audio signals are complex, continuous-valued waves. In order to save a digital version, we sample the signal and convert it into numbers. For example, speech is commonly sampled at 44,100 Hz. This means that each second of the signal is broken down into 44,100 parts, and the values at these timestamps are stored. In other words, you store a value every 1/44,100 seconds. As the sampling rate is high, we feel that the signal is continuous when we listen to it on our media players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will use the `wavfile` package to read the audio file from `.wav` input file, so we will draw the signal with a diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will code in the `read_plot.py` file and use the wavfile package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wave audio files are uncompressed files. The format was introduced with Windows 3.1 as a standard format for the sound used in multimedia applications. Its technical specifications and description can be found in the Multimedia Programming Interface and Data Specifications 1.0 document (https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf). It is based on the Resource Interchange File Format (RIFF) specifications that were introduced in 1991, constituting a metaformat for multimedia files running in the Windows environment. The RIFF structure organizes blocks of data in sections called chunks, each of which describes a characteristic of the WAV file (such as the sample rate, the bit rate, and the number of audio channels), or contains the values of the samples (in this case, we are referring to chunk data). The chunks are 32 bit (with some exceptions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the WAV file, the `scipy.io.wavfile.read()` function was used. This function returns **data from a WAV file** along with **the sample rate**. The returned sample rate is a **Python integer**, and the **data is returned as a NumPy array** with a datatype that corresponds to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming audio signals into the frequency domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio signals consist of a complex mixture of **sine waves of different frequencies**, **amplitudes**, and **phases**. Sine waves are also referred to as **sinusoids**. There is **a lot of information that is hidden in the frequency content of an audio signal**. In fact, **an audio signal is heavily characterized by its frequency content**. The whole world of speech and music is based on this fact. Before you proceed further, **you will need some knowledge of Fourier transforms**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "we will see how to **transform an audio signal into the frequency domain**. To do this, the numpy.fft.fft() function is used. This function computes the one-dimensional n-point **discrete Fourier transform (DFT)** with the efficient **fast Fourier transform (FFT)** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "We'll code in the `freq_transform.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "\n",
    "The sound spectrum is a graphical representation of the sound level, normally in **decibels (dB)**, depending on the frequency in Hz. If the sound to be analyzed is a so-called pure sound (signal at a single frequency constant over time), for example, a perfect sine wave, the signal spectrum will have a single component at the sine wave frequency, with a certain level in dB. In reality, any real signal consists of a large number of sinusoidal components of amplitude that are continuously variable over time. For these signals, it is impossible to analyze pure tones because there are always fractions of the signal energy that are difficult to represent with sinusoids. In fact, the representation of a signal as the sum of sinusoidal harmonic components, according to the Fourier transform theorem, is only valid for stationary signals, which often do not correspond to real sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "<t style=\"color: green\"> The frequency analysis of the sounds is based on the Fourier transform theorem</t>. That is,<t style=\"color: red\"> any periodic signal can be generated by summing together so many sinusoidal signals (called harmonics) having multiple whole frequencies of the frequency of the periodic signal (called fundamental frequency)</t>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating audio signals with custom parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sound is a particular type of wave in which a variation of pressure that is induced by a vibrating body (that is, a sound source) propagates in the surrounding medium (usually air). Some examples of sound sources include the following:\n",
    "\n",
    "* Musical instruments in which the vibrating part can be a struck string (such as a guitar), or rubbed with a bow (such as the violin).\n",
    "* Our vocal cords that are made to vibrate from the air that comes out of the lungs and give rise to the voice.\n",
    "* Any phenomenon that causes a movement of air (such as the beating wings of a bird, an airplane that breaks down the supersonic barrier, a bomb that explodes, or a hammer beating on an anvil) having appropriate physical characteristics.\n",
    "\n",
    "To reproduce sound through electronic equipment, it is necessary to transform it into an analogue sound that is an electric current that originates from the transformation by conversion of the mechanical energy of the sound wave into electrical energy. In order to be able to use the sound signals with the computer, it is necessary to transfigure the analogue in a digital signal originating from the transformation of the analog sound into an audio signal represented by a flow of 0 and 1 (bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "we will use NumPy to generate audio signals. As we discussed earlier, audio signals are complex mixtures of sinusoids. So, we will bear this in mind when we generate our own audio signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to generate audio signals with custom parameters. We'll code in the `generate.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it work\n",
    "In this recipe, we used the NumPy library to generate audio signals. We have seen that a digital sound is a sequence of numbers, so generating a sound will be enough to build an array that represents a musical tone. First, we set the filename to where the output will be saved. Then, we specified the audio parameters. Thus, we generated audio using a sine wave. We then added some noise, so we resized to 16-bit integer values. In the end, we wrote the signal on the output file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "In the coding of a signal, each value assigned to the single sample is represented in bits. Each bit corresponds to a dynamic range of 6 dB. The higher the number of bits used, the higher the range of dB that can be represented by the single sample.\n",
    "\n",
    "Some of the typical values are as follows:\n",
    "\n",
    "* 8 bits per sample that correspond to $256$ levels.\n",
    "* 16 bits per sample (the number used for CDs) that correspond to $65,636$ levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesizing music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional musical instruments, sound is produced by the vibration of mechanical parts. In synthetic instruments, vibration is described by functions over time, called signals, which express the variation in the time of the acoustic pressure. Sound synthesis is a process that allows you to generate the sound artificially. The parameters by which the timbre of the sound is determined differ according to the type of synthesis that is used for the generation, and can be provided directly by the composer, or with actions on appropriate input devices, or derived from the analysis of pre-existing sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will see how to synthesize some music. To do this, we will use various notes, such as A, G, and D, along with their corresponding frequencies, to generate some simple music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "We'll code is in the `synthesize_music.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Music is a work of ingenuity and creativity that is difficult to explain in a nutshell. Musicians read a piece of music recognizing the notes as they are placed on the stave. By analogy, we can regard the synthesis of sound as a sequence of the characteristic frequencies of the known ones. In this recipe, we have used this procedure to synthesize a short sequence of notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "To generate music artificially, the synthesizer is used. All synthesizers have the following basic components that work together to create a sound:\n",
    "\n",
    "* An oscillator that generates the waveform and changes the tone\n",
    "* A filter that cuts out some frequencies in the wave to change the timbre\n",
    "* An amplifier that controls the volume of the signal\n",
    "* A modulator to create effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting frequency domain features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Transforming audio signals into the frequency domain recipe, we discussed how to convert a signal into the frequency domain. In most modern speech recognition systems, people use frequency domain features. After you convert a signal into the frequency domain, you need to convert it into a usable form. **Mel Frequency Cepstral Coefficients (MFCC)** is a good way to do this. MFCC takes the power spectrum of a signal and then uses a combination of filter banks and **discrete cosine transform (DCT)** to extract the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    " We will see how to use the `python_speech_features` package to **extract frequency domain features**. You can find the installation instructions at http://python-speech-features.readthedocs.org/en/latest. So, let's take a look at how to extract MFCC features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "We'll work in the `extract_freq_features.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The **cepstrum** is the **result of the Fourier transform applied to the dB spectrum of a signal**. Its name is derived from the reversal of the first four letters of the word **spectrum**. It was defined in 1963 by Bogert et al. Thus, the cepstrum of a signal is the Fourier transform of the log value of the Fourier transform of the signal. \n",
    "\n",
    "The graph of the cepstrum is used to analyze the rates of change of the spectral content of a signal. Originally, it was invented to analyze earthquakes, explosions, and the responses to radar signals. It is currently a very effective tool for discriminating the human voice in music informatics. For these applications, the spectrum is first transformed through the frequency bands of the Mel scale. The result is the spectral coefficient Mel, or MFCCs. It is used for voice identification and pitch detection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "**The cepstrum is used to separate the part of the signal that contains the excitation information from the transfer function performed by the larynx**. The lifter action (filtering in the frequency domain) has as its objective the separation of the excitation signal from the transfer function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to discuss speech recognition. We will use HMMs to perform speech recognition; **HMMs are great at modeling time series data**. **As an audio signal is a time series signal**, **HMMs perfectly suit our needs**.<t style=\"color: green\"> An HMM is a model that represents probability distributions over sequences of observations </t>. We assume that **the outputs are generated by hidden states**. So, our goal is to **find these hidden states so that we can model the signal**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "we will see how to build an HMM using the `hmmlearn` package. Before you proceed, you will need to install the `hmmlearn` package. Let's take a look at how to build HMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to buil Hidden Markov Models. We'll work in the `speech_recognizer.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "HMM is **a model where the system is assumed to be a Markov process with unobserved states**. A **stochastic process** is called **Markovian** when, having chosen a certain instance of $t$ for observation, the evolution of the process, starting with $t$, <t style=\"color : green\">depends only on $t$</t>, and <t style=\"color : red\">does not depend in any way on the previous instances</t>. Thus, **a process is Markovian** when, **given the moment of observation, only a particular instance determines the future evolution of the process, and that evolution does not depend on the past**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "An HMM is, therefore, a Markov chain in which states are not directly observable. More precisely, it can be understood as follows:\n",
    "\n",
    "* The chain has a number of states\n",
    "* The states evolve according to a Markov chain\n",
    "* Each state generates an event with a certain probability distribution that depends only on the state\n",
    "* The event is observable, but the state is not\n",
    "HMMs are particularly known for their applications in the recognition of the temporal pattern of spoken speeches, handwriting, texture recognition, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a speech recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech recognition is the **process by which human oral language is recognized, and subsequently processed through a computer, or, more specifically, through a special speech recognition system**. Speech recognition systems are used for automated voice applications in the context of telephone applications (such as automatic call centers) for dictation systems, which allow the dictation of speeches to the computer, for control systems of the navigation system satellite, or for a phone in a car via voice commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a database of speech files to build our speech recognizer. We will use the database available at https://code.google.com/archive/p/hmm-speech-recognition/downloads. This contains 7 different words, where each word has 15 audio files associated with it. Download the ZIP file and extract the folder that contains the Python file (rename the folder that contains the data as data). This is a small dataset, but it is sufficient in understanding how to build a speech recognizer that can recognize 7 different words. We need to build an HMM model for each class. When we want to identify the word in a new input file, we need to run all the models on this file and pick the one with the best score. We will use the HMM class that we built in the previous recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "In this recipe, we created a speech recognition system using an HMM. To do this, we **first created a function to analyze input arguments**. Then, **a class was used to handle all HMM-related processing**. Thus, **we have classified the input data and then predicted the label of the test data**. Finally, we **printed the results**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "A *voice recognition* system is **based on a comparison of the input audio**, which is **appropriately processed**, **with a database created during system training**. In practice, **the software application tries to identify the word spoken by the speaker, looking for a similar sound in the database, and checking which word corresponds**. Naturally, **it is a very complex operation**. Moreover, **it is not done on whole words, but on the phonemes that compose them**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a TTS system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speech synthesis** is the technique that is used for the artificial reproduction of the human voice. A system used for this purpose is called a **speech synthesizer** and can be implemented by software or hardware. Speech synthesis systems are also known as **TTS systems due to their ability to convert text into speech**. There are also systems that **convert phonetic symbols into speech**.\n",
    "\n",
    "Speech synthesis can be **achieved by concatenating recordings of vocals stored in a database**. The various systems of speech synthesis **differ according to the size of the stored voice samples**. That is, `a system that stores single phonemes or double phonemes allows you to obtain the maximum number of combinations at the expense of overall clarity`, while other systems which are designed for a specific use repeat themselves, to record whole words or entire sentences in order to achieve a high-quality result.\n",
    "\n",
    "A synthesizer can create a **completely synthetic voice using vocal traits and other human characteristics**. The `quality of a speech synthesizer is evaluated on the basis of both the resemblance to the human voice and its level of comprehensibility`. A TTS conversion program with good performance can play an important role in accessibility; for example, **by allowing people with impaired vision or dyslexia to listen to documents written on the computer**. For this type of application (since the early 1980s), many operating systems have included speech synthesis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will introduce the Python library that allows us to create TTS systems. We will run the `pyttsx` cross-platform TTS wrapper library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a TTS system in the `tts.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "A speech synthesis system or engine is composed of two parts: a frontend and a backend.\n",
    "* The frontend part deals with the conversion of the text into phonetic symbols,\n",
    "* while the backend part interprets the phonetic symbols and reads them, thus, transforming them into an artificial voice.\n",
    "\n",
    "The frontend has two key functions; first, it performs **an analysis of the written text to convert all numbers, abbreviations, and abbreviations into words in full**. This preprocessing step is referred to as **tokenization**. The second function consists of **converting each word into its corresponding phonetic symbols and performing the linguistic analysis of the revised text, subdividing it into prosodic units**, that is, into *prepositions, sentences,* and *periods*. The **process of assigning phonetic transcription to words** is called **conversion from text to phoneme, or from grapheme to phoneme**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "An evolution of the classic TTS system is called **WaveNet**, and it seems to **know how to speak, articulate accents, and pronounce a whole sentence fluently**. **WaveNet is a deep neural network that generates raw audio**. It was created by researchers at the London-based artificial intelligence firm, DeepMind. WaveNet uses a **deep generative model for sound waves that can imitate any human voice**. The sentences pronounced by WaveNet sound 50% more similar to a human voice than the more advanced TTS. To demonstrate this, samples were created in English and Mandarin, and using the **Mean Opinion Scores (MOS) system**, which is now a **standard in audio evaluation, samples of artificial intelligence were compared to those generated by normal TTS, parametric-TTS, and also with respect to the samples of real voices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ac4613ae4247844c12aaa6a0684fc01719fcfaa01de7cc764ac1c94816adf3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
