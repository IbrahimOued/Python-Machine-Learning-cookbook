{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Array in python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We'll cover an array creation procedure. We'll fist create an array using numpy and then display its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3. , -1.5,  2. , -5.4],\n",
       "       [ 0. ,  4. , -0.3,  2.1],\n",
       "       [ 1. ,  3.3, -1.9, -4.3]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([[3, -1.5, 2, -5.4], [0, 4, -0.3, 2.1], [1, 3.3, -1.9, -4.3]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "numpy provides us various tools for creating an array. For ex, to create on-D array of equidistant values with numbers from 0 to 10, we should use the ```arange()``` function, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npArray1 = np.arange(10)\n",
    "npArray1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a numeric array from 0 to 50, with a step of 5 (using predetermined step between successive values), we will write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90,\n",
       "       95])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npArray2=np.arange(10, 100, 5)\n",
    "npArray2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to create a $1$-D array of 50 numbers between 2 limit values and that are equidistant in this range, we will use the ```linspace()``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.20408163,  0.40816327,  0.6122449 ,  0.81632653,\n",
       "        1.02040816,  1.2244898 ,  1.42857143,  1.63265306,  1.83673469,\n",
       "        2.04081633,  2.24489796,  2.44897959,  2.65306122,  2.85714286,\n",
       "        3.06122449,  3.26530612,  3.46938776,  3.67346939,  3.87755102,\n",
       "        4.08163265,  4.28571429,  4.48979592,  4.69387755,  4.89795918,\n",
       "        5.10204082,  5.30612245,  5.51020408,  5.71428571,  5.91836735,\n",
       "        6.12244898,  6.32653061,  6.53061224,  6.73469388,  6.93877551,\n",
       "        7.14285714,  7.34693878,  7.55102041,  7.75510204,  7.95918367,\n",
       "        8.16326531,  8.36734694,  8.57142857,  8.7755102 ,  8.97959184,\n",
       "        9.18367347,  9.3877551 ,  9.59183673,  9.79591837, 10.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npArray3 = np.linspace(0, 10, 50)\n",
    "npArray3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data preprocessing *using mean removal***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, we usually have to deal with a lot of raw data. This raw data is not readily ingestible by machine learning algorithms. To prepare data for machine learning, we have to preprocess it before we feed it into various algorithms.\\\n",
    "This is an intensive process that takes plenty of time, almost $80\\%$ of the entire data analysis process, in some scenarios. However, it is vital for the rest of the data analysis workflow, so it is necessary to learn the best practices of these techniques. Before sending our data to any machine learning algorithm, we need to cross check the quality and accuracy of the data. If we are unable to reach the data stored in Python correctly, or if we can't switch from raw data to something that can be analyzed, we cannot go ahead.\\\n",
    "Data can be preprocessed in many ways—***standardization, scaling, normalization, binarization, and one-hot encoding*** are some examples of preprocessing techniques. We will address them through simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<t style=\"color: yellow\">Standardization or mean removal</t> is a technique that simply **centers data by removing the average value of each characteristic, and then scales it by dividing non-constant characteristics by their standard deviation**. It's usually beneficial to remove the mean from each feature so that it's centered to $0$. This helps us remove bias from features. The formula used to achieve this is:\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}} = \\frac{\\text{s} - \\text{mean}}{\\text{sd}}\n",
    "$$\n",
    "\n",
    "Standardization results in **the rescaling of features**, which in turn represents of a **standard normal distribution** $\\text{mean} = 0$, $\\text{sd} = 1$. In this formula, mean is the *mean* and *sd* is the standard deviation of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Let's import sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 1.33333333  1.93333333 -0.06666667 -2.53333333]\n",
      "Standard deviation:  [1.24721913 2.44449495 1.60069429 3.30689515]\n"
     ]
    }
   ],
   "source": [
    "# 2 To understand the outcome of mean removal on our data, which can ve a sequence\n",
    "print(\"Mean: \", data.mean(axis=0))\n",
    "print(\"Standard deviation: \", data.std(axis=0))\n",
    "# or an iterator. The std() function returns the standard deviation, a measure of the\n",
    "# distribution of the array elements. The axis parameter specifies the axis along which\n",
    "# these functions are compured 0 for columns and 1 for rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Now, let's perform the standardization\n",
    "data_standardized = preprocessing.scale(data)\n",
    "# The preprocessing.scale() function standardizes the dataset along any axis. This method centers the\n",
    "# data on the mean and resizes the components in order to have a unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]\n",
      "Standard deviation:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 4 Now we recalculate the mean and standard deviation on the standardized data\n",
    "print(\"Mean: \", data_standardized.mean(axis=0))\n",
    "print(\"Standard deviation: \", data_standardized.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the *mean* is almost $0$ and the *standard deviation* is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The `sklearn.preprocessing` package provides several common utility functions and transformer classes to modify the features available in a representation that best suits our needs. In this recipe, the `scale()` function has been used **(z-score standardization)**.  In summary, the `z-score` (also called the standard score) represents **the number of standard deviations by which the value of an observation point or data is greater than the mean value of what is observed or measured**. **Values more than the mean have positive z-scores**, while **values less than the mean have negative z-scores**. The z-score is a quantity without dimensions that is obtained by **subtracting the population's mean from a single rough score and then dividing the difference by the standard deviation of the population**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Standardization is particularly useful when **we do not know the minimum and maximum for data distribution**. In this case, it is not possible to use other forms of data transformation. As a result of the transformation, the normalized values do not have a minimum and a fixed maximum. Moreover, **this technique is not influenced by the presence of outliers, or at least not the same as other methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Scaling**\n",
    "The values of each feature in a dataset can vary between random values. So, sometimes it is important to scale them so that this becomes a level of playing field. Through this statistical procedure, it's possible to compare identical variables belonging to different distributions and diffrent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: It is a good practice to rescale data before training a ml algorithm. With rescaling, data units are eliminated, allowing you to easily compare data from different locations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We'll use the **min-max** method(usually called **feature scaling**) to get all of the scaled data in the range $[0, 1]$. The formula used to achieve this is as follows:\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}} = \\frac{\\text{s} - x_{min}}{x_{max} - x_{min}}\n",
    "$$\n",
    "\n",
    "To **scale features between a given minimum and maximum** value—in our case, between 0 and 1—so that the maximum absolute value of each feature is scaled to unit size, the ```preprocessing.MinMaxScaler()``` function can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to scale data in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [ 0.  -1.5 -1.9 -5.4]\n",
      "Max:  [ 0.  -1.5 -1.9 -5.4]\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's start by defining the data_scaler variable\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "# 2 Now we will use the fit_transform() method, which fits the data and then transforms\n",
    "# it(we will use the same data as in the previous recipe)\n",
    "data_scaled = data_scaler.fit_transform(data)\n",
    "# A numpy array of a specific shape is returned. To understand how this function has transformed\n",
    "# data, we display the min and max of each column in the array\n",
    "\n",
    "# 3 First, for the starting data and then for the processed data\n",
    "print(\"Min: \", data.min(axis=0))\n",
    "print(\"Max: \", data.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  [0. 0. 0. 0.]\n",
      "Max:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Now let's do the samed forprint(\"Min: \", data.min(axis=0))\n",
    "print(\"Max: \", data_scaled.min(axis=0))\n",
    "print(\"Max: \", data_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         1.         0.        ]\n",
      " [0.         1.         0.41025641 1.        ]\n",
      " [0.33333333 0.87272727 0.         0.14666667]]\n"
     ]
    }
   ],
   "source": [
    "# After scaling, all the feature values range between the specific values\n",
    "# 5 to display the scaled array, we will use\n",
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the data is included in the same interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works...\n",
    "When data has different range, **the impact on response variables might be higher than the one with a lesser numeric range**, which can affect the prediction accuracy. Our goal is to improve predictive accuracy and ensure this doesn't happen. Hence, we may need to scale values under different features so that they fall within a similar range. Through this statistical procedure, it's possible to compare identical variables belonging to different distributions and different variables or variables expressed in different units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Feature scaling consists of limiting the excursion of a set of values within a certain predefined interval. It guarantees that all functionalities have the exact same scale, but does not handle anomalous values well. This is because extreme values become the extremes of the new range of variation. In this way, the actual values are compressed by keeping the distance to the anomalous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Normalization**\n",
    "Data normalization is used **when you want to adjust the values in the feature vector so that they can be measured on a common scale**. One of the most common forms of normalization that is used in ML adjusts the values of a feature vector so that they sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize datan the `preprocessing.normalize()` function can be used. It **scales input vectors individually to a unit form (vector length)**. Three types of norms are provided, `l1, l2` or `max`, and they are explained next. If $x$ is the vector of covariates of length $n$, the normalized vector is $y = \\frac{x}{z}$ where $z$ is defined as follows\n",
    "\n",
    "$$\n",
    "l1: z = \\sum^{n}_{i}|x_i|\n",
    "$$\n",
    "\n",
    "$$\n",
    "l2: z = \\sqrt{\\sum^{n}_{i}x^2_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "max: z = max(x_i)\n",
    "$$\n",
    "\n",
    "The **norm** is a function that assigns a positive length to each vetor belonging to a vector space, except $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75       -0.17045455  0.47619048 -0.45762712]\n",
      " [ 0.          0.45454545 -0.07142857  0.1779661 ]\n",
      " [ 0.25        0.375      -0.45238095 -0.36440678]]\n"
     ]
    }
   ],
   "source": [
    "# 1 As said, to normalize data, the preprocessing.normalize() function can be used\n",
    "# as follows (we will use the same data as in the previous recipe)\n",
    "data_normalized = preprocessing.normalize(data, norm='l1', axis=0)\n",
    "\n",
    "# 2 To display the normalized array, we will use the following code\n",
    "print(data_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# This is used a lot to make sure that datasets don't get boosted artificially due to\n",
    "# the funcdamendal nature of their features\n",
    "\n",
    "# 3 As already mentionned, the normalized array along the columns (features) must reutrn a sum\n",
    "# equal to 1, let's check for each column\n",
    "data_norm_abs = np.abs(data_normalized)\n",
    "print(data_norm_abs.sum(axis=0))\n",
    "# In the first line of code, we used the np.abs() function to evaluate the absolute value\n",
    "# of each element in the array. In the second row of code, we used the sum() function to\n",
    "# calculate the sum of each column (axis=0). The following results are returned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the sum of the absolute value of the elements of each column is equal to 1, so the data is normalized.\n",
    "\n",
    "##### How it works\n",
    "In this recipe, we normalized the data at our disposal to the unitary norm. Each sample with at least one non-zero component was rescaled independently of other samples so that its norm was equal to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Scalling inputs to a unit norm is a very common task in text classification and clustering problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Binarization**\n",
    "Binarization is used when you want to **convert a numerical feature vector into a boolean vector**. In the field of digital image processing, image binarization is the process by which a color or grayscale image is transformed into a binary image, that is, an image with only $2$ color (back and white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "This technique is used for the recognition of objects, shapes and specifically, characters. **Through binarization, it is possible to distinguish the object of interest from the background on which it is found**. **Skeletonization** is instead an essential and schematic representation of the object, which generally preludes the subsequent real recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 To binarize data, we will use the preprocessing.Binarizer() function as follows\n",
    "# on the same data\n",
    "data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```preprocessing.Binarizer()``` function binarizes data according to an imposed ```threshold``` map to $0$. With the default *threshold* of $0$, only positive values map to 1. In our case, the *threshold* imposed is $1.4$, so values **greater than $1.4$ are mapped to $1$, while values less are mapped to $0$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 2 To display the binarized array\n",
    "print(data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very useful technique that's usually used when we have some prior knowledge of the data\n",
    "\n",
    "##### How it works\n",
    "The fundamental idea of ​​this technique is to draw a fixed demarcation line. It is therefore a matter of finding an appropriate threshold and affirming that all the points of the image whose light intensity is below a certain value belong to the object (background), and all the points with greater intensity belong to the background (object).\n",
    "\n",
    "\n",
    "##### There's more\n",
    "Binarization is a widespread operation on count data, in which the analyst can decide to consider only the presence or absence of a characteristic rather than a quantified number of occurrences. Otherwise, it can be used as a preprocessing step for estimators that consider random Boolean variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **One-Hot encoding**\n",
    "We often deal with numerical values that are sparse and scattered all over the place. We don't really need to store these values. This is where one-hot encoding comes into the picture. **We can think of one-hot encoding as a tool that tightens feature vectors**. It **looks at each feature and identifies the total number of distinct values**. It uses a one-of-k scheme to encode values. Each feature in the feature vector is encoded based on this scheme. This helps us to be more efficient in terms of space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's say we are dealing with $4$-dimensional feature vectors. To encode the $n^{th}$ feature in a feature vector, the encoder will go through the $n^{th}$ feature in each feature vector and count the number of distinct values. If the number of distinct values is $k$, it will transform the feature into a k-dimensional vector where only one value is $1$ and all other values are $0$. Let's take a simple example to understand how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2]\n",
      " [0 2 3]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's take an array with 4 rows (vectors) and 4 columns (features)\n",
    "data = np.array([[1, 1, 2], [0, 2, 3], [1, 0, 1], [0, 1, 0]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the values present in each column (feature):\n",
    "\n",
    "* The first feature has two possible values: $0, 1$\n",
    "* The second feature has three possible values: $0, 1, 2$\n",
    "* The third feature has four possible values: $0, 1, 2, 3$\n",
    "So, overall, **the sum of the possible values present in each feature is given by** $2 + 3 + 4 = 9$. This means that **$9$ entries are required to uniquely represent any vector**. The three features will be represented as follows:\n",
    "\n",
    "* Feature $1$ starts at index $0$\n",
    "* Feature $2$ starts at index $2$\n",
    "* Feature $3$ starts at index $5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# To encode categorical integer features as a one-hot numeric array, the preprocessing.OneHotEncoder() function\n",
    "# can be used as follows\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoder.fit(data)\n",
    "\n",
    "# The first row of code sets the encoder, then the fit() function fits the OneHotEncoder object to a data array\n",
    "# 3 Now we can transform the data array using one hot encoding. To do so, the\n",
    "# transform() function will be used as follows\n",
    "encoded_vector = encoder.transform([[1, 2, 3]]).toarray()\n",
    "print(encoded_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is clear: the first feature (1) has an index of 1, the second feature (3) has an index of 4, and the third feature (3) has an index of 8. As we can verify, only these positions are occupied by a 1; all the other positions have a 0. Remember that Python indexes the positions starting from 0, so the 9 entries will have indexes from 0 to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "The `preprocessing.OneHotEncoder()` function encodes categorical integer features as a one-hot numeric array. Starting from an array of integers or strings that denotes the values assumed by categorical characteristics (discrete), this function encodes the characteristics using a one-hot coding scheme, returning dummy variables. This creates a binary column for each category and returns a sparse array or a dense array.\n",
    "\n",
    "##### There's more\n",
    "It often happens that you have to convert categorical data. This is due to the fact that many machine learning algorithms can't work directly with categorical data. To use these methods, it is necessary to first transform categorical data into numerical data. This is required for both input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Label encoding**\n",
    "In SL, we usually deal with a variety of labels. These can be either numbers or words. If they are numbers, then the algorithm can use them directly. However, labels often need to be in a human readable form. So, people usually label the training data with words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Label encoding refers to **transforming word labels into a numerical form so that algorithms can understand how to operate on them**. Let's take a look at how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: \n",
      "audi ---> 0\n",
      "bmw ---> 1\n",
      "ford ---> 2\n",
      "toyota ---> 3\n"
     ]
    }
   ],
   "source": [
    "# 1 import the preprocessing package\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 2 The package contains various functions that are needed for data\n",
    "# processing. To encode labels with a value between 0 and n_classes-1, the\n",
    "# preprocessing.labelEncoder() function can be used. Let's define the label encoder as follows\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# 3 The label_encoder object knows how to understand words labels. Let's create some labels:\n",
    "input_classes = ['audi', 'ford', 'audi', 'toyota', 'ford', 'bmw']\n",
    "\n",
    "# 4 We are now ready to encode these labels-first, the fit() function is used to fit the label encoder\n",
    "# and then the class mapping encoder are printed\n",
    "label_encoder.fit(input_classes)\n",
    "print(\"Class mapping: \")\n",
    "for i, item in enumerate(label_encoder.classes_):\n",
    "    print(item, '--->', i)\n",
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels =  ['toyota', 'ford', 'audi']\n",
      "Encoded labels =  [3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 6 As shown in the preceding output, the words have been transformed into zero indexed numbers. Now, when you encounter a set of labels,\n",
    "# you can simply transform them as follows\n",
    "labels = ['toyota', 'ford', 'audi']\n",
    "encoded_labels = label_encoder.transform(labels)\n",
    "print(\"labels = \", labels)\n",
    "print(\"Encoded labels = \", list(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels = [2, 1, 0, 3, 1]\n",
      "Decoded labels = ['ford', 'bmw', 'audi', 'toyota', 'bmw']\n"
     ]
    }
   ],
   "source": [
    "# This is way easier than mannualy maintaining mapping between words\n",
    "# and numbers. You can check the correcteness by transforming numbers back into words labes\n",
    "encoded_labels = [2, 1, 0, 3, 1]\n",
    "decoded_labels = label_encoder.inverse_transform(encoded_labels)\n",
    "print(\"Encoded labels =\", encoded_labels)\n",
    "print(\"Decoded labels =\", list(decoded_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform labels back to their original encoding, the `inverse_transform()` function has been applied.\n",
    "\n",
    "##### How it works...\n",
    "In this recipe, we used the `preprocessing.LabelEncoder()` function to transform word labels into numerical form. To do this, we first set up a series of labels to as many car brands. We then turned these labels into numerical values. Finally, to verify the operation of the procedure, we printed the values corresponding to each class labeled.\n",
    "\n",
    "##### There's more\n",
    "In the last two recipes, Label encoding and One-hot encoding, we have seen how to transform data. Both methods are suitable for dealing with categorical data. But what are the pros and cons of the two methodologies? Let's take a look:\n",
    "\n",
    "* **Label encoding** can *transform categorical data into numeric data*, but the *imposed ordinality creates problems if the obtained values are submitted to mathematical operations*.\n",
    "* **One-hot encoding** has the *advantage that the result is binary rather than ordinal*, and that everything is in an orthogonal vector space. The disadvantage is that *for high cardinality, the feature space can explode*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a Linear regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression refers to finding the underlying function with the help of linear combination of input variables. The previous example had an input variable and an output variable. A simple linear regression is easy to understand, but represents the basis of regression techniques. Once these concepts are understood, it will be easier for us to address the other types of regression.\n",
    "\n",
    "The linear regression method consists of precisely identifying a line that is capable of representing point distribution in a two-dimensional plane, that is, if the points corresponding to the observations are near the line, then the chosen model will be able to describe the link between the variables effectively. \n",
    "\n",
    "In theory, there are an infinite number of lines that may approximate the observations, while in practice, there is only one mathematical model that optimizes the representation of the data. In the case of a linear mathematical relationship, the observations of the y variable can be obtained by a linear function of the observations of the x variable. For each observation, we will use the following formula:\n",
    "\n",
    "$y = \\alpha \\times x + \\beta$\n",
    "\n",
    "In the preceding formula, x is the explanatory variable and y is the response variable. The α and β parameters, which represent the slope of the line and the intercept with the y-axis respectively, must be estimated based on the observations collected for the two variables included in the model.\n",
    "\n",
    "The slope, $\\alpha$, is of particular interest, that is, the variation of the mean response for every single increment of the explanatory variable. What about a change in this coefficient? If the slope is positive, the regression line increases from left to right, and if the slope is negative, the line decreases from left to right. When the slope is zero, the explanatory variable has no effect on the value of the response. But it is not just the sign of α that establishes the weight of the relationship between the variables. More generally, its value is also important. In the case of a positive slope, the mean response is higher when the explanatory variable is higher, while in the case of a negative slope, the mean response is lower when the explanatory variable is higher.\n",
    "\n",
    "The main aim of linear regression is to get the underlying linear model that connects the input variable to the output variable. This in turn reduces the sum of squares of differences between the actual output and the predicted output using a linear function. This method is called ordinary least squares. In this method, the coefficients are estimated by determining numerical values that minimize the sum of the squared deviations between the observed responses and the fitted responses, according to the following equation:\n",
    "\n",
    "$RSS = \\sum^{n}_{i=1}(\\alpha \\times x_i + \\beta - y_i)^2$\n",
    "\n",
    "This quantity represents the sum of the squares of the distances to each experimental datum $(x_i, y_i)$ from the corresponding point on the straight line.\n",
    "\n",
    "You might say that there might be a curvy line out there that fits these points better, but linear regression doesn't allow this. The main advantage of linear regression is that it's not complex. If you go into non-linear regression, you may get more accurate models, but they will be slower. As shown in the preceding diagram, the model tries to approximate the input data points using a straight line. Let's see how to build a linear regression model in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression **is used to find ut the relationship between input data and the continuously-valued output data. This is generally represented as real numbers, and our aim is to estimate the core function that calculates the mapping from the input to the output.**\n",
    "You have been provided with a data file called VehiclesItaly.txt. This contains comma-separated lines, where the first element is the input value and the second element is the output value that corresponds to this input value. Our goal is to find the linear regression relation between the vehicle registrations in a state and the population of a state. You should use this as the input argument. As anticipated, the Registrations variable contains the number of vehicles registered in Italy and the Population variable contains the population of the different regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to build a linear regressor in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the regressor.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Computing regression accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to build a regressor, it's important to understand how to evaluate the quality of a regressor as well. In this context, **an error is defined as the difference between the actual value and the value that is predicted by the regressor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's quickly take a look at the metrics that can be used to measure the quality of a regressor. A regressor can be evaluated using many different metrics. There is a module in the scikit-learn library that provides functionalities to compute all the following metrics. This is the `sklearn.metrics` module, which includes **score functions**, **performance metrics**, **pairwise metrics**, and **distance computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to compute regression accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See regressor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regressor can be evaluated using many different metrics, such as the following:\n",
    "\n",
    "* **Mean absolute error**: This is the average of absolute errors of all the data points in the given dataset.\n",
    "* **Mean squared error**: This is the average of the squares of the errors of all the data points in the given dataset. It is one of the most popular metrics out there!\n",
    "* **Median absolute error**: This is the median of all the errors in the given dataset. The main advantage of this metric is that it's robust to outliers. A single bad point in the test dataset wouldn't skew the entire error metric, as opposed to a mean error metric.\n",
    "* **Explained variance score**: This score measures how well our model can account for the variation in our dataset. A score of $1.0$ indicates that our model is perfect.\n",
    "* **R2 score**: This is pronounced as R-squared, and this score refers to the coefficient of determination. This tells us how well the unknown samples will be predicted by our model. The best possible score is $1.0$, but the score can be negative as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "The sklearn.metrics module contains a series of simple functions that measure prediction error:\n",
    "* Functions ending with `_score` return a value to maximize, the higher the better\n",
    "* Functions ending with `_error` or `_loss` return a value to minimize, the lower the better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Achieving Model Persistance** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model, it would be nice if we could save it as a file so that it can be used later by simply loading it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's see how to achieve model persistence programmatically. To do this, the pickle module can be used. The pickle module is used to store Python objects. This module is a part of the standard library with your installation of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see regressor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle module transforms an **arbitrary Python object into a series of bytes**. This process is also called the **serialization of the object**. The byte stream representing the object can be transmitted or stored, and subsequently rebuilt to create a new object with the same characteristics. The inverse operation is called **unpickling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "In Python, there is also another way to perform serialization, **by using the marshal module**. In general, the pickle module is recommended **for serializing Python objects**. The marshal module can be used to support Python `.pyc` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Buildind a ridge regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main problems of linear regression is that it's sensitive to outliers. During data collection in the real world, it's quite common to wrongly measure output. Linear regression uses ordinary least squares, which tries to minimize the squares of errors. The outliers tend to cause problems because they contribute a lot to the overall error. This tends to disrupt the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are values that, compared to others, are particularly extreme (values that are clearly distant from the other observations). Outliers are an issue because they might distort data analysis results; more specifically, descriptive statistics and correlations.\n",
    "\n",
    "Outliers can be univariate when they have an extreme value for a single variable, or multivariate when they have a unique combination of values for a number of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `regularization` method involves modifying the performance function, normally selected as the sum of the squares of regression errors on the training set. When a large number of variables are available, the least square estimates of a linear model often have a low bias but a high variance with respect to models with fewer variables. Under these conditions, there is an overfitting problem. To improve precision prediction by allowing greater bias but a small variance, we can use variable selection methods and dimensionality reduction, but these methods may be unattractive for computational burdens in the first case or provide a difficult interpretation in the other case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to address the problem of overfitting is to modify the estimation method by neglecting the requirement of an unbiased parameter estimator and instead considering the possibility of using a biased estimator, which may have smaller variance. There are several biased estimators, most of which are based on regularization: Ridge, Lasso, and ElasticNet are the most popular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "**Ridge regression** is a regularization method where **a penalty is imposed on the size of the coefficients**. As we said in the Building a linear regressor section, in the ordinary least squares method, the coefficients are estimated by determining numerical values that minimize the sum of the squared deviations between the observed responses and the fitted responses, according to the following equation:\n",
    "\n",
    "$RSS = \\sum^{n}_{i=1}(y_i - \\beta_1 * x_i + \\beta_2)^2$\n",
    "\n",
    "**Ridge regression**, in order to estimate the $\\beta$ coefficients, starts from the basic formula of the **residual sum of squares (RSS) and adds the penalty term**. $\\lambda$ ($\\ge 0$) is defined as **the tuning parameter**, which is multiplied **by the sum of the $\\beta$ coefficients squared (excluding the intercept)** to define the penalty period, as shown in the following equation:\n",
    "\n",
    "$\\sum^{n}_{i=1}(y_i - \\beta_1 * x_i + \\beta_2)^2 + \\lambda * \\beta^2_1 = RSS + \\lambda * \\beta^2_1$\n",
    "\n",
    "It is evident that having $\\lambda = 0$ means **not having a penalty in the model**, that is, **we would produce the same estimates as the least squares**. On the other hand, having a $\\lambda$ tending toward infinity means **having a high penalty effect**, which will bring many coefficients close to zero, but will not imply their exclusion from the model. Let's see how to build a ridge regressor in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a ridge regressor in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `regressor.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression is a regularization method** where a **penalty is imposed on the size of the coefficients**. **Ridge regression is identical to least squares**, barring the fact that **ridge coefficients are computed by decreasing a quantity that is somewhat different**. In ridge regression, a scale transformation has a substantial effect. Therefore, to avoid obtaining different results depending on the predicted scale of measurement, **it is advisable to standardize all predictors before estimating the model**. To standardize the variables, we must *subtract their means and divide by their standard deviations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a polynomial regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main constraints of a linear regression model is the fact that **it tries to fit a linear function to the input data**. The polynomial regression model overcomes this issue **by allowing the function to be a polynomial**, thereby increasing the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Polynomial models should be applied **where the relationship between response and explanatory variables is curvilinear**. Sometimes, polynomial models can also be used **to model a non-linear relationship in a small range of explanatory variable**. A polynomial quadratic (squared) or cubic (cubed) term **converts a linear regression model into a polynomial curve**. However, since it is the explanatory variable that is squared or cubed and not the beta coefficient, it is still considered as a linear model. This makes it a simple and easy way to model curves, without needing to create big non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curviness of a model is controlled by the degree of the polynomial. As the curviness of the model increases, it gets more accurate. However, **curviness adds complexity to the model as well, making it slower**. This is a trade-off: **you have to decide how accurate you want your model to be given the computational constraints**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to build a polynomial regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 n this example, we will only deal with second-degree parabolic\n",
    "# regression. Now, we'll show how to model data with a polynomial.\n",
    "# We measured the temperature for a few hours of the day. We want\n",
    "# to know the temperature trend even at times of the day when we\n",
    "# did not measure it. Those times are, however, between the initial\n",
    "# time and the final time at which our measurements took place:\n",
    "import numpy as np\n",
    "\n",
    "Time = np.array([6, 8, 11, 14, 16, 18, 19])\n",
    "Temp = np.array([4, 7, 10, 12, 11.5, 9, 7])\n",
    "\n",
    "# 2 Now, we will show the temperature at a few points during the day:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(Time, Temp, 'bo')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Temp\")\n",
    "plt.title('Temperature versus time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we analyze the graph, it is possible to note a curvilinear pattern of the data that can be modeled through a second-degree polynomial such as the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Temp = \\beta_0 + \\beta_1 * Time + \\beta_2 * Time^2 $\n",
    "\n",
    "The unknown coefficients, $\\beta_0$, $\\beta_1$ and $\\beta_2$, are estimated by **decreasing the value of the sum of the squares**. This is obtained by **minimizing the deviations of the data from the model to its lowest value (least squares fit)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the polynomial coefficients\n",
    "beta = np.polyfit(Time, Temp, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy.polyfit()` function returns the coefficients for a polynomial of degree $n$ (given by us) that is the best fit for the data. The coefficients returned by the function are in descending powers (highest power first), and their length is $n+1$ if $n$ is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, let's verify that it actually fits our data. To do this, use the model to evaluate the polynomial at uniformly spaced times. To evaluate the model at the specified points, we can use the `poly1d()` function. This function returns the value of a polynomial of degree n evaluated at the points provided by us. The input argument is a vector of length $n+1$ whose elements are the coefficients in descending powers of the polynomial to be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.poly1d(beta)\n",
    "# As you can see in the upcoming graph, this is\n",
    "# close to the output value. If we want it to get closer,\n",
    "# we need to increase the degree of the polynomial.\n",
    "\n",
    "# 5 Now we are plotting the original data and the model on the same plot\n",
    "xp = np.linspace(6, 19, 100)\n",
    "plt.figure()\n",
    "plt.plot(Time, Temp, 'bo', xp, p(xp), '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we analyze the graph, we can see that the curve fits our data sufficiently.\n",
    "This model fits the data to a greater extent than a simple linear\n",
    "regression model. In regression analysis, it's important to keep the order\n",
    "of the model as low as possible. In the first analysis, we keep the model\n",
    "as a first order polynomial. If his is not satisfactory, then a second-order\n",
    "polynomial is tried. The use of higher-order polynomials can lead to incorrect evaluations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Polynomial regression **should be used when linear regression is not good enough**. With polynomial regression, **we approached a model in which some predictors appear in degrees equal to or greater than two** to fit the data with a curved line. Polynomial regression is usually used when **the relationship between variables looks curved**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "At what degree of the polynomial must we stop? It **depends on the degree of precision we are looking for**. **The higher the degree of the polynomial, the greater the precision of the model**, but the **more difficult it is to calculate**. In addition, it is necessary to verify the significance of the coefficients that are found, but let's get to it right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to apply our knowledge to a real-world problem. Let's apply all these principles to estimate house prices. This is one of the most popular examples that is used to understand regression, and it serves as a good entry point. This is intuitive and relatable, hence making it easier to understand the concepts before we perform more complex things in machine learning. We will use a decision tree regressor with AdaBoost to solve this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "A **decision tree** is a tree where each node makes a simple decision that contributes to the final output. The leaf nodes represent the output values, and the branches represent the intermediate decisions that were made, based on input features. **Adaboost** stands for **adaptive boosting**, and this is a technique that is used to boost the accuracy of the results from another system. This combines the outputs from different versions of the algorithms, called **weak learners**, using a weighted summation to get the final output. The information that's collected at each stage of the AdaBoost algorithm is fed back into the system so that the learners at the latter stages focus on training samples that are difficult to classify. In this way, it increases the accuracy of the system.\n",
    "\n",
    "Using Adaboost, we fit a regressor on the dataset. We compute the error and then fit the regressor on the same dataset again, based on this error estimate. We can think of tis as fine-tuning on the regressor until the desired accuracy is achieved. You are given a dataset that contains various parameters that affect the price of a house. Our goal is to estimate the relationship between these parameters and the house price so that we can use this to estimate the price given unknown input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to estimate housing prices in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Create a new file called housing.py and add the lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see housing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data point has 12 input parameters that affect the price of a house. You can access the input data using housing_data.data and the corresponding price using housing_data.target. The following attributes are available:\n",
    "\n",
    "* `crim`: Per capita crime rate by town\n",
    "* `zn`: Proportion of residential land zoned for lots that are over 25,000 square feet\n",
    "* `indus`: Proportion of non-retail business acres per town\n",
    "* `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* `nox`: Nitric oxides concentration (parts per ten million)\n",
    "* `rm`: Average number of rooms per dwelling\n",
    "* `age`: Proportion of owner-occupied units built prior to 1940\n",
    "* `dis`: Weighted distances to the five Boston employment centers\n",
    "* `rad`: Index of accessibility to radial highways\n",
    "* `tax`: Full-value property-tax rate per $10,000\n",
    "* `ptratio`: Pupil-teacher ratio by town\n",
    "* `lstat`: Percent of the lower status of the population\n",
    "* `target`: Median value of owner-occupied homes in $1000 \n",
    "\n",
    "Of these, `target` is the **response variable**, while the other 12 variables are **possible predictors**. The goal of this analysis is to fit a regression model that best explains the variation in target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "**`DecisionTreeRegressor`** builds a decision tree regressor.  Decision trees are used to predict a response or class $y$, from several input variables; $x_1, x_2,…,x_n$. If $y$ is a **continuous response**, **it's called a regression tree**, if y is **categorical**, **it's called a classification tree**. The algorithm is based on the following procedure:\n",
    "\n",
    "* We see the value of the input *x_i$ at each node of the tree, and based on the answer, we continue to the left or to the right branch.\n",
    "* When we reach a leaf, we will find the prediction. In regression trees, we try to divide the data space into tiny parts, where we can equip a simple different model on each of them. The non-leaf part of the tree is just the way to find out which model we will use for predicting it.\n",
    "\n",
    "A regression tree is formed by a series of nodes that split the root branch into two child branches. Such subdivision continues to cascade. Each new branch, then, can go in another node, or remain a leaf with the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "An AdaBoost regressor is a meta-estimator that starts by equipping a regressor on the actual dataset and adding additional copies of the regressor on the same dataset, but where the weights of instances are adjusted according to the error of the current prediction. As such, consecutive regressors look at difficult cases. This will help us compare the results and see how AdaBoost really boosts the performance of a decision tree regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the relative importance of features\n",
    "Are all features equally important? In this case, we used 13 input features, and they all contributed to the model. However, an important question here is, How do we know which features are more important? **Obviously, not all features contribute equally to the output**. In case we want to discard some of them later, **we need to know which features are less important**. We have this functionality available in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "Let's calculate the **relative importance of the features**. Feature importance provides a measure that indicates the value of each feature in the construction of the model. **The more an attribute is used to build the model, the greater its relative importance**. This importance is **explicitly calculated for each attribute in the dataset**, allowing you to classify and compare attributes to each other.  Feature importance is an attribute contained in the model (`feature_importances_`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to extract this. Let's continue with `housing.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Feature importance provides a measure that indicates the value of each feature in the construction of a model. The more an attribute is used to build a model, the greater its relative importance. In this recipe, the feature_importances_ attribute was used to extract the relative importance of the features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Relative importance returns the utility of each characteristic in the construction of decision trees. The more an attribute is used to make predictions with decision trees, the greater its relative importance. This importance is explicitly calculated for each attribute in the dataset, allowing you to classify and compare attributes to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating bicycle demand distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a different regression method to solve the bicycle demand distribution problem. We will use the random forest regressor to estimate the output values. **A random forest is a collection of decision trees**. This **basically uses a set of decision trees that are built using various subsets of the dataset**, and then **it uses averaging to improve the overall performance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "We will use the bike_day.csv file that is provided to you. This is also available [here](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). There are 16 columns in this dataset. The first two columns correspond to the serial number and the actual date, so we won't use them for our analysis. The last three columns correspond to different types of outputs. The last column is just the sum of the values in the fourteenth and fifteenth columns, so we can leave those two out when we build our model. Let's go ahead and see how to do this in Python. We will analyze the code line by line to understand each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how to estimate bicycle demande distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 We first need to import a couple of new packages\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# 2 We are processing a CSV file, so the CSV package for useful in handling these files\n",
    "# let's import the data into the environment\n",
    "filename = \"bike_day.csv\"\n",
    "file_reader = csv.reader(open(filename, 'r'), delimiter=',')\n",
    "X, y = [], []\n",
    "for row in file_reader:\n",
    "    X.append(row[2: 15])\n",
    "    y.append(row[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code just read all the data from the CSV file. The `csv.reader()` function returns a reader object, which will iterate over lines in the given CSV file. Each row read from the CSV file is returned as a list of strings. So, two lists are returned: X and y. We have separated the data from the output values and returned them. Now we will extract feature names:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature names are useful when we display them on a graph. So, we have to remove the first row from X and y because they are feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X[1:]).astype(np.float32)\n",
    "y=np.array(y[1:]).astype(np.float32)\n",
    "\n",
    "# We have also converted the two lists into two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, n_estimators=1000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 Let's shuffle these two arrays to make them\n",
    "# independent of the order in which the data is arranged in the file:\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y, random_state=7)\n",
    "\n",
    "# 4 As we did earlier, we need to separate the data into\n",
    "# training and testing data. This time, let's use 90% of the data\n",
    "# for training and the remaining 10% for testing:\n",
    "num_training = int(0.9 * len(X))\n",
    "X_train, y_train = X[:num_training], y[:num_training]\n",
    "X_test, y_test = X[num_training:], y[num_training:]\n",
    "\n",
    "# 5 Let's go ahead and train the regressor:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=1000, max_depth=10, min_samples_split=2)\n",
    "rf_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RandomForestRegressor()` function builds a random forest regressor. Here, n_estimators refers to the number of estimators, which is the number of decision trees that we want to use in our random forest. The max_depth parameter refers to the maximum depth of each tree, and the min_samples_split parameter refers to the number of data samples that are needed to split a node in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Random Forest regressor performance ####\n",
      "Mean squared error = 21396.29\n",
      "Explained variance score = 0.99\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate the performance of the random forest regressor:\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "evs = explained_variance_score(y_test, y_pred)\n",
    "print( \"#### Random Forest regressor performance ####\")\n",
    "print(\"Mean squared error =\", round(mse, 2))\n",
    "print(\"Explained variance score =\", round(evs, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's extract the relative importance of the features:\n",
    "RFFImp= rf_regressor.feature_importances_ \n",
    "RFFImp= 100.0 * (RFFImp / max(RFFImp))\n",
    "index_sorted = np.flipud(np.argsort(RFFImp))\n",
    "pos = np.arange(index_sorted.shape[0]) + 0.5\n",
    "\n",
    "# To visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.bar(pos, RFFImp[index_sorted], align='center')\n",
    "plt.xticks(pos, feature_names[index_sorted])\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title(\"Random Forest regressor\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "**A random forest is a special regressor formed of a set of simple regressors (decision trees)**, represented as **independent and identically distributed random vectors**, where each one chooses the mean prediction of the individual trees. This type of structure has made significant improvements in regression accuracy and falls within the sphere of ensemble learning. Each tree within a random forest is constructed and trained from a random subset of the data in the training set. The trees therefore do not use the complete set, and the best attribute is no longer selected for each node, but the best attribute is selected from a set of randomly selected attributes.\n",
    "\n",
    "Randomness is a factor that then becomes part of the construction of regressors and aims to increase their diversity and thus reduce correlation. The final result returned by the random forest is nothing but the average of the numerical result returned by the different trees in the case of a regression, or the class returned by the largest number of trees if the random forest algorithm was used to perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "Let's see what happens when you include the fourteenth and fifteenth columns in the dataset. In the feature importance graph, every feature other than these two has to go to zero. The reason is that the output can be obtained by simply summing up the fourteenth and fifteenth columns, so the algorithm doesn't need any other features to compute the output. Make the following change inside the for loop (the rest of the code remains unchanged):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "X.append(row[2:15])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot the feature importance graph now, you will see the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it says that only these two features are important. This makes sense intuitively because the final output is a simple summation of these two features. So, there is a direct relationship between these two variables and the output value. Hence, the regressor says that it doesn't need any other variable to predict the output. This is an extremely useful tool to eliminate redundant variables in your dataset. But this is not the only difference from the previous model. If we analyze the model's performance, we can see a substantial improvement:\n",
    "```py\n",
    "#### Random Forest regressor performance ####\n",
    "Mean squared error = 21396.29\n",
    "Explained variance score = 0.99\n",
    "```\n",
    "We therefore have 99% of the variance explained: a very good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5052bdfd4928f465c492d21ff3d7daf709b24acb81685a4e868a1ee9dbe33934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
