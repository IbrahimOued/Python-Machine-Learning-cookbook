{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning represents a family of algorithms that are able to learn and adapt to environmental changes. It is based on the concept of receiving external stimuli based on the choices of the algorithm. A correct choice will result in a reward, while a wrong choice will lead to a penalty. The goal of the system is to achieve the best possible result.\n",
    "\n",
    "In supervised learning, the correct output is clearly specified (learning with a teacher). But it is not always possible to do so. Often, we only have qualitative information. The information that's available is called a **reinforcement signal**. In these cases, the system does not provide any information on how to update the agent's behavior (for example, weights). You cannot define a cost function or a gradient. The goal of the system is to create the smart agents that are able to learn from their experience.\n",
    "\n",
    "In the following screenshot, we can see a flowchart that displays the reinforcement learning interaction with the environment:\n",
    "\n",
    "![environment](rl_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps to follow to correctly apply a reinforcement learning algorithm:\n",
    "\n",
    "1. Preparation of the agent\n",
    "2. Observation of the environment\n",
    "3. Selection of the optimal strategy\n",
    "4. Execution of actions\n",
    "5. Calculation of the corresponding reward (or penalty)\n",
    "6. Development of updating strategies (if necessary)\n",
    "7. Repetition of steps 2 to 5 iteratively until the agent learns the optimal strategies\n",
    "\n",
    "Reinforcement learning tries to maximize the rewards that are received for the execution of the action or set of actions that allow a goal to be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather forecasting with MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid load problems and computational difficulties, the agent-environment interaction is considered a **Markov decision process (MDP)**. An MDP is a discrete time stochastic control process.\n",
    "\n",
    "**Stochastic processes** are mathematical models that are used to study the evolution of phenomena following random or probabilistic laws. It is known that in all natural phenomena, both by their very nature and by observational errors, a random or accidental component is present.\n",
    "\n",
    "This component causes the following: at every instance of $t$, the result of the observation of the phenomenon is a random number or random variable, $st$. It is not possible to predict with certainty what the result will be; you can only state that it will take one of several possible values, each of which has a given probability.\n",
    "\n",
    "A stochastic process is called **Markovian** when, having chosen a certain instance of t for observation, the evolution of the process, starting with t, depends only on t and does not depend in any way on the previous instances. Thus, a process is Markovian when, given the moment of observation, only this instance determines the future evolution of the process, while this evolution does not depend on the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we want to build a statistical model to predict the weather. To simplify the model, we will assume that there are only two states: sunny and rainy. Let's further assume that we have made some calculations and discovered that tomorrow's time is somehow based on today's time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to it\n",
    "Let's see how to perform weather forecasting with MDP. We'll work in the `MarkovChain.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "A Markov chain is a mathematical model of a random phenomenon that evolves over time in such a way that the past influences the future only through the present. In other words, a stochastic model describes a sequence of possible events where the probability of each event depends only on the state that was attained in the previous event. So, Markov chains have the property of memorylessness.\n",
    "\n",
    "The structure of a Markov chain is therefore completely represented by the following transition matrix:\n",
    "\n",
    "$p = \\begin{bmatrix}\n",
    "p_{11} & p_{12} & \\dots & p_{1n}\\\\\n",
    "p_{21} & p_{22} & \\dots & p{2n}\\\\\n",
    "\\dots & \\dots & \\dots & \\dots\\\\\n",
    "p_{n1} & p_{n2} & \\dots & p_{nn}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The properties of transition probability matrices derive directly from the nature of the elements that compose them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "A very intuitive alternative to the description of a Markov chain through a transition matrix is associating an oriented graph (transition diagram) to a Markov chain. The transition matrix and transition diagram provide the same information regarding the same Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing a financial portfolio using DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The management of financial portfolios is an activity that aims to combine financial products in a manner that best represents the investor's needs. This requires an overall assessment of various characteristics, such as risk appetite, expected returns, and investor consumption, as well as an estimate of future returns and risk. **Dynamic programming (DP)** represents a set of algorithms that can be used to calculate an optimal policy given a perfect model of the environment in the form of an MDP. The fundamental idea of DP, as well as reinforcement learning in general, is the use of state values and actions to look for good policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting ready\n",
    "In this recipe, we will adress the **Knapsack problem**: A thief goes into a house and wants to steal valuables. They put them in their knapstack, but they are limited by the weight. Each object has its own value and weight. They must choose the objects that are of value, but that do not have excessive weight. The thief must not exceed the weight limit in the knapstack, but, at the same time, they must optimize their gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to do it\n",
    "Let's see how we can optimize a financial portofolio using DP.\n",
    "We'll use the `KPDP.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How it works\n",
    "Consider, for example, the problem of finding the best path that joins two locations. The principle of optimality states that each sub path included in it, between any intermediate location and the final location, must in turn be optimal. Based on this principle, DP solves a problem by taking one decision at a time. At every step, the best policy for the future is determined, regardless of the past choices (it is a Markov process), assuming that the latter choices are also optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There's more\n",
    "DP is a technique for solving recursive problems more efficiently. Why is this the case? Oftentimes, in recursive procedures, we solve sub problems repeatedly. In DP, this does not happen: we memorize the solution of these sub problems so that we do not have to solve them again. This is called **memoization**. If the value of a variable at a given step depends on the results of previous calculations, and if the same calculations are repeated over and over, then it is convenient to store the intermediate results so as to avoid repeating computationally expensive calculations."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95569d36909702d70226d3f7da3963104e10d4f9f573cf419503c1a4a79c4a35"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
